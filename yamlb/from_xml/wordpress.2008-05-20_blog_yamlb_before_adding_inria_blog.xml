<?xml version="1.0" encoding="UTF-8"?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your blog. -->
<!-- It contains information about your blog's posts, comments, and categories. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your blog. -->

<!-- To import this information into a WordPress blog follow these steps. -->
<!-- 1. Log into that blog as an administrator. -->
<!-- 2. Go to Manage: Import in the blog's admin panels. -->
<!-- 3. Choose "WordPress" from the list. -->
<!-- 4. Upload this file using the form provided on that page. -->
<!-- 5. You will first be asked to map the authors in this export file to users -->
<!--    on the blog.  For each author, you may choose to map to an -->
<!--    existing user on the blog or to create a new user -->
<!-- 6. WordPress will then import each of the posts, comments, and categories -->
<!--    contained in this file into your blog -->

<!-- generator="WordPress/MU" created="2008-05-20 22:16"-->
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.0/"
>

<channel>
	<title>Yet Another Machine Learning Blog</title>
	<link>http://yamlb.wordpress.com</link>
	<description></description>
	<pubDate>Wed, 14 Mar 2007 15:53:23 +0000</pubDate>
	<generator>http://wordpress.org/?v=MU</generator>
	<language>en</language>
	<wp:wxr_version>1.0</wp:wxr_version>
	<wp:base_site_url>http://wordpress.com/</wp:base_site_url>
	<wp:base_blog_url>http://yamlb.wordpress.com</wp:base_blog_url>
	<wp:category><wp:category_nicename>blogroll</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Blogroll]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>decision</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Decision]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>events</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Events]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>inference</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Inference]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>misc</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Misc]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>modeling</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Modeling]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>philosophy</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[philosophy]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>priors</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Priors]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>research-life</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[research life]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>uncategorized</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Uncategorized]]></wp:cat_name></wp:category>
	<wp:category><wp:category_nicename>work</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Work]]></wp:cat_name></wp:category>
		<item>
<title>RSS reading groups</title>
<link>http://yamlb.wordpress.com/?p=7</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=7</guid>
<description></description>
<content:encoded><![CDATA[to do]]></content:encoded>
<wp:post_id>7</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Dictretisation</title>
<link>http://yamlb.wordpress.com/?p=8</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=8</guid>
<description></description>
<content:encoded><![CDATA[@INPROCEEDINGS{Fayyad93multi,
author = {U. M. Fayyad and K. B. Irani},
title = {Multi-Interval Discretization of Continuous-Valued Attributes for
Classification Learning},
booktitle = {Proc. of the 13th IJCAI},
year = {1993},
pages = {1022-1027},
address = {Chambery, France},
}

Puis

@INPROCEEDINGS{dougherty95disc,
author = {James Dougherty and Ron Kohavi and Mehran Sahami},
title = {Supervised and Unsupervised Discretization of Continuous Features},
booktitle = {International Conference on Machine Learning},
year = {1995},
pages = {194-202},
pdf = {dougherty95disc.pdf},
url = {citeseer.ist.psu.edu/dougherty95supervised.html},
}

Puis ce que j ai ecrit sur les choix d r dans un histogramme
choisr la discretisation c est choisri la mesure de base et ceci peut etre fait par invariances ?]]></content:encoded>
<wp:post_id>8</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Temporal bayesian updating</title>
<link>http://yamlb.wordpress.com/?p=9</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=9</guid>
<description></description>
<content:encoded><![CDATA[Bayes rule isn't temporal.

But before/after prior/posterior anticipating collecting data gathering information
reverse in time ?]]></content:encoded>
<wp:post_id>9</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>KL divergence</title>
<link>http://yamlb.wordpress.com/?p=10</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=10</guid>
<description></description>
<content:encoded><![CDATA[The <a target="_blank" title="kl" href="http://eom.springer.de/K/k110180.htm">Kullback and Leibler divergence</a> is a common measure of the "distance" between two probability distributions. It's central in machine learning algorithm based on probabilities.

For instance, when trying to approximate a distribution p(x), we can try minimize KL(p,q) with q belonging to a particular class of distributions (ex: exponential family).

This is used in a lot of variational methods and approximate message passing algorithms.

But why using this divergence (which isn't a real distance)? Why not using L^2 distance ? Or Chi-square ?

I found several leads:
<ul>
	<li><strong>Information geometry: </strong>KL is a special case of <a title="information geometry" target="_blank" href="http://citeseer.ist.psu.edu/zhu95information.html">delta-divergences</a>. These divergences have the great advantage to be invariant by reparametrisation.<strong>
</strong></li>
	<li><strong>Information theory: </strong>KL can be seen as the amount of information (in bits) missing to q in order to specify p. (conditional entropy). It is the average "surprise" of a incoming message drawn from q when you expect it arrived form p.</li>
	<li><strong>Bayesian theory: </strong>KL minimisation can be derived from log-likelihood maximisation.</li>
</ul>
Invariance seems to be the more general requirement because it leads to a family of divergences. Exchanging KL with a delta-divergence in our algorithm can give a <a title="minka paper" target="_blank" href="http://research.microsoft.com/~minka/papers/message-passing/">better understanding</a> on what's really going on.

The information theory justification seems weaker to me because it takes place in a theory of communication, and it requires a subjective receiver.

Finally I'm still not sure of my derivation from Log Likelihood minimisation, especially in the continous case.]]></content:encoded>
<wp:post_id>10</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>HyperPriors</title>
<link>http://yamlb.wordpress.com/?p=11</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=11</guid>
<description></description>
<content:encoded><![CDATA[What's hyper priors?
What's hyper ' hyper ' hyper ' hyper ' hyper ' hyper ' hyper priors?

Are the necessary? (see <a href="http://citeseer.ist.psu.edu/zhu95information.html">zhu </a>paper)]]></content:encoded>
<wp:post_id>11</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>What is &#8220;to understand&#8221;?</title>
<link>http://yamlb.wordpress.com/?p=12</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=12</guid>
<description></description>
<content:encoded><![CDATA[barlow

delahaye

Kolmogoroph

chaitin]]></content:encoded>
<wp:post_id>12</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Develeopmental robotics</title>
<link>http://yamlb.wordpress.com/?p=13</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=13</guid>
<description></description>
<content:encoded><![CDATA[data mining, principled approqach to ML, including bayesian and learning theory, optimal processing of available information

other hand when will we see real robots. Biological inspiration, analagogy, heuristic, no one understnad how the pb is solved. Fro instnca GA, but failed to solve moderate size problems (Koza)

Developmenatl robotics

It is not knowledge, but the act of learning, not possession but the act of
getting there, which grants the greatest enjoyment. When I have clarified
and exhausted a subject, then I turn away from it, in order to go into
darkness again; the never-satisfied man is so strange if he has completed a
structure, then it is not in order to dwell in it peacefully, but in order
to begin another. I imagine the world conqueror must feel thus, who, after
one kingdom is scarcely conquered, stretches out his arms for others.
Gauss, Karl Friedrich
Letter to Bolyai, 1808.

You know that I write slowly. This is chiefly because I am never satisfied
until I have said as much as possible in a few words, and writing briefly
takes far more time than writing at length.
Gauss, Karl Friedrich
In G. Simmons Calculus Gems, New York: McGraw Hill inc., 1992.]]></content:encoded>
<wp:post_id>13</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Feyman diagram are marginalisation</title>
<link>http://yamlb.wordpress.com/?p=14</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=14</guid>
<description></description>
<content:encoded><![CDATA[]]></content:encoded>
<wp:post_id>14</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>time and prior/posterior</title>
<link>http://yamlb.wordpress.com/?p=15</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=15</guid>
<description></description>
<content:encoded><![CDATA[]]></content:encoded>
<wp:post_id>15</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Conditionning is projection</title>
<link>http://yamlb.wordpress.com/?p=16</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=16</guid>
<description></description>
<content:encoded><![CDATA[cf math blog
<h1><span class="d-msg" /> <a href="http://www.dcorfield.pwp.blueyonder.co.uk/blog.html"> <strong>Philosophy of Real Mathematics</strong></a><a href="http://www.dcorfield.pwp.blueyonder.co.uk/blog.html"> </a></h1>]]></content:encoded>
<wp:post_id>16</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Who wants to be a millionnaire?</title>
<link>http://yamlb.wordpress.com/?p=17</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=17</guid>
<description></description>
<content:encoded><![CDATA[http://arxiv.org/PS_cache/physics/pdf/0504/0504185.pdf

<font size="2" /><font size="2" face="Arial" /><font face="Verdana, Arial, Helvetica, sans-serif">1 chance sur </font><font color="#ff0000">13 983 816</font> de trouver 6 numéros<font size="2" face="Verdana, Arial, Helvetica, sans-serif">  </font>

<font size="2" /><font size="2" face="Arial">post de gelman</font>

<font size="2" /><font size="2" face="Arial">Ainsi, en quelques heures de travail, je  révèle un  des secrets les mieux  gardés du LOTO, à savoir les numéros    les plus joués : 7 9 13 12 11 4 3 5 6 8 ..., et  les numéros  les moins joués, donc les plus  intéressants : 39 41 40 47 35 37 32 30 ...</font>

<font size="2" /><font size="2" face="Arial">http://attimont.club.fr/loto/loto.htm</font>

<img src="///C:/DOCUME%7E1/t-pierrd/LOCALS%7E1/Temp/moz-screenshot.jpg" /><img align="bottom" src="http://www.stat.columbia.edu/~cook/movabletype/archives/natural_numbers.png" />]]></content:encoded>
<wp:post_id>17</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>The Hutter Prize for Lossless Compression of Human Knowledge</title>
<link>http://yamlb.wordpress.com/?p=18</link>
<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=18</guid>
<description></description>
<content:encoded><![CDATA[<div style="font-family:0;font-size:12px;" class="moz-text-plain">
<pre>The Hutter Prize for Lossless Compression of Human Knowledge

Researchers in artificial intelligence are being put to the test by a
new competition:  The Hutter Prize.

The Hutter Prize challenges researchers to achieve ever shorter
descriptions of the first 100MB of Wikipedia, the collaborative online
encyclopedia. These descriptions are required to be executable on
widely available personal computers, and to exactly reproduce that
sample of Wikipedia within practical computation resources.

The purpose of this competition is to advance the field of artificial
intelligence through the compression of human knowledge, represented
by a substantial part of Wikipedia. The better one can compress
natural language, the better one understands it, the better on can
predict; and being able to predict well is key for being able to act
intelligently.

The principle that less is more was first proposed by William of
Ockham in the 13th century, formalized as universal inductive
inference by Ray Solomonoff, as shortest codes by Andrey Kolmogorov
and Gregory Chaitin, and as minimal descriptions by Chris Wallace and
Jorma Rissanen during the 1960s and 70s. In the 2000s, Marcus Hutter
unified this development with decision theory to prove that an
artificial agent's optimal action is to maximize its total reward
based on the shortest model consistent with its past observations.
Finding this model is incomputable, but, improving upon Levin search,
he also devised an asymptotically fastest algorithm for this and a
large class of other problems.

The Hutter Prize fund currently stands at 50,000 euro and is awarded
in increments with improvements in compression ratio of the Wikipedia
sample.

This incentive works.  In the first month of the Hutter Prize,
contestants improved on the previous best text compression program by
nearly 6%.

It recently appeared at Slashdot and other websites and received a lot
of attention.

For details of the Hutter Prize see:

<a class="moz-txt-link-freetext" href="http://prize.hutter1.net/">http://prize.hutter1.net</a>

For discussion of the Hutter Prize see:

<a class="moz-txt-link-freetext" href="http://groups.google.com/group/Hutter-Prize">http://groups.google.com/group/Hutter-Prize</a>

-- James Bowery
_______________________________________________
Kolmogorov mailing list
<a class="moz-txt-link-abbreviated" href="mailto:Kolmogorov@idsia.ch">Kolmogorov@idsia.ch</a>
<a class="moz-txt-link-freetext" href="http://mailman.ti-edu.ch/mailman/listinfo/kolmogorov">http://mailman.ti-edu.ch/mailman/listinfo/kolmogorov</a></pre>
</div>]]></content:encoded>
<wp:post_id>18</wp:post_id>
<wp:post_date>0000-00-00 00:00:00</wp:post_date>
<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Introduction - todo list</title>
<link>http://yamlb.wordpress.com/2005/12/15/introduction-todo-list/</link>
<pubDate>Thu, 15 Dec 2005 12:13:21 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2005/12/15/introduction-todo-list/</guid>
<description></description>
<content:encoded><![CDATA[This is Yet Another Blog on Machine Learning, and Bayesian Stuff.

Things to do:
* List other ml blogs
* List other ml reading groups (voir un des blogs y*.blogspot)
* List of web sites (http://www.mlnet.org/ )
* List of software packages (BNT, proBt, bayesia, WEKA, Torch...)]]></content:encoded>
<wp:post_id>19</wp:post_id>
<wp:post_date>2005-12-15 13:13:21</wp:post_date>
<wp:post_date_gmt>2005-12-15 12:13:21</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>introduction-todo-list</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>2</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-04 10:18:01</wp:comment_date>
<wp:comment_date_gmt>2006-03-04 09:18:01</wp:comment_date_gmt>
<wp:comment_content><![CDATA[About a year ago I did a google search and collected all up-to-date Machine Learning reading groups. So the list is out of date now, and it'd be good if someone made a fresh one, here's it for reference
http://yaroslavvb.blogspot.com/2005/02/machine-learning-reading-groups.html]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Who&#8217;s writing that?</title>
<link>http://yamlb.wordpress.com/2006/02/15/whos-writing-that/</link>
<pubDate>Wed, 15 Feb 2006 15:47:24 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Uncategorized]]></category>

		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/15/whos-writing-that/</guid>
<description></description>
<content:encoded><![CDATA[<a href="http://emotion.inrialpes.fr/~dangauthier/">Me!</a>]]></content:encoded>
<wp:post_id>20</wp:post_id>
<wp:post_date>2006-02-15 16:47:24</wp:post_date>
<wp:post_date_gmt>2006-02-15 15:47:24</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>whos-writing-that</wp:post_name>
<wp:status>static</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:postmeta>
<wp:meta_key>_wp_page_template</wp:meta_key>
<wp:meta_value>default</wp:meta_value>
</wp:postmeta>
	</item>
<item>
<title>Other blogs on the web</title>
<link>http://yamlb.wordpress.com/2006/02/15/other-blogs-on-the-web/</link>
<pubDate>Wed, 15 Feb 2006 15:54:43 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/15/other-blogs-on-the-web/</guid>
<description></description>
<content:encoded><![CDATA[I read ML blogs on the web, all really interesting:
<ul>
	<li><a href="http://hunch.net">hunch.net</a> John Langford</li>
	<li><a href="http://ml.typepad.com">ml.typepad.com</a> Olivier Bousquet</li>
	<li><a href="http://yaroslavvb.blogspot.com">yaroslavvb.blogspot.com</a> Yaroslav Bulatov</li>
</ul>

I found others, but not updated. Am I missing interesting ones ?




]]></content:encoded>
<wp:post_id>21</wp:post_id>
<wp:post_date>2006-02-15 16:54:43</wp:post_date>
<wp:post_date_gmt>2006-02-15 15:54:43</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>other-blogs-on-the-web</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>3</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-04 09:49:05</wp:comment_date>
<wp:comment_date_gmt>2006-03-04 08:49:05</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Another machine learning blog, cool!
(we can never have too many :P)

BTW, there's also 
a statistics blog http://www.stat.columbia.edu/~cook/movabletype/mlm/

learning in Vision
http://vimsu99.blogspot.com/

and Thesilog
http://thesilog.sologen.net/

and a few semi-related 
http://bloglines.com/public/yaroslavvb]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>ML readings groups</title>
<link>http://yamlb.wordpress.com/2006/02/15/ml-readings-groups/</link>
<pubDate>Wed, 15 Feb 2006 15:57:41 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/15/ml-readings-groups/</guid>
<description></description>
<content:encoded><![CDATA[<a href="http://yaroslavvb.blogspot.com">Yaroslav Bulatov</a> has made a list of <a href="http://yaroslavvb.blogspot.com/2005/02/machine-learning-reading-groups.html">ML reading groups</a> one year ago. Some links could be broken or outdated, but most of them are still valid.

]]></content:encoded>
<wp:post_id>22</wp:post_id>
<wp:post_date>2006-02-15 16:57:41</wp:post_date>
<wp:post_date_gmt>2006-02-15 15:57:41</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>ml-readings-groups</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>People, Conf, and journals</title>
<link>http://yamlb.wordpress.com/2006/02/15/people-conf-and-journals/</link>
<pubDate>Wed, 15 Feb 2006 16:19:25 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/15/people-conf-and-journals/</guid>
<description></description>
<content:encoded><![CDATA[Who are the main authors in bayesian ML ?
What are the main conferences and journals.
I list what I have in mind at this moment.

<strong>Where to start</strong>
If you want to start with bayesian ml, I propose to look at
<ul>
	<li><a href="http://www.gatsby.ucl.ac.uk/%7Ezoubin/course05/index.html">UCL course (Ghahramani)</a></li>
	<li><a href="http://research.microsoft.com/~minka/papers/">Papers from T.Minka</a></li>
</ul>
<strong>Authors</strong>
<ul>
	<li>Zoubin Ghahramani</li>
	<li>Radford Neal </li>
	<li>Tom Minka</li>
	<li>David McKay</li>
</ul>
<strong>Conferences</strong>
<ul>
	<li>NIPS</li>
	<li>CVPR</li>
	<li>UAI</li>
	<li>ICML</li>
</ul>
<strong>Journals</strong>
<ul>
	<li>JMLR</li>
	<li>Machine Learning</li>
	<li>Bayesian Analysis</li>
</ul>

]]></content:encoded>
<wp:post_id>23</wp:post_id>
<wp:post_date>2006-02-15 17:19:25</wp:post_date>
<wp:post_date_gmt>2006-02-15 16:19:25</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>people-conf-and-journals</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>MaxLikelihood doesn&#8217;t exists !</title>
<link>http://yamlb.wordpress.com/2006/02/15/maxlikelihood-doesnt-exists/</link>
<pubDate>Wed, 15 Feb 2006 16:38:50 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Priors]]></category>

		<category domain="category" nicename="priors"><![CDATA[Priors]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/15/maxlikelihood-doesnt-exists/</guid>
<description></description>
<content:encoded><![CDATA[After a discussion with <a href="http://www.inrialpes.fr/movi/people/gargallo/">Pau</a>, it appears to me that the conventional <strong>Maximum Likelihood</strong> point estimation doesn't exists, I mean that it is actually a <strong>Maximum A Posteriori</strong> with an uniform prior.

Why am I stressing this, although it's obvious as soon as you studied probabilities ? Because I think that people who use ML methods are thinking they don't use any prior, that they are "objective", and that, if they have a problem, that can't be the fault of a bad prior, simply because they don't have any prior.

But actually, the assumption of a uniform prior <strong>is</strong> a big assumption. It's indeed often admitted that there is no such things as "informative priors", and at least   the flat one isn't non-informative at all.

In the correct bayesian method, one should consider carefully his priors, so a approach that consider by default a flat prior isn't following this correct approach.

In other word, maximising the likelihood of a model as no fundamental justification, the likelihood is just a function, and doesn't represents any sort of "belief" or "probability". The only correct way is to think to it in terms of posterior with flat prior. Likelihood maximisation was a heuristic in orthodox statistics (for example in parameter estimation), and this is all it can be.

Once you have a good prior, then you can think that your posterior is the product of a prior and a likelihood, and then you can think to those 2 terms as being some function of the model P(M|D)=CST* P(D|M) P(M)= CST * f1(M) * f2(M). Then its correct to reason on them, for instance for thinking to the so-called <strong>automatic Occam razor</strong>, who'll be the subject of another post.
]]></content:encoded>
<wp:post_id>24</wp:post_id>
<wp:post_date>2006-02-15 17:38:50</wp:post_date>
<wp:post_date_gmt>2006-02-15 16:38:50</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>maxlikelihood-doesnt-exists</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>4</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-02-16 13:55:28</wp:comment_date>
<wp:comment_date_gmt>2006-02-16 12:55:28</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Pau, you disagree because of the non existance of flat prior on infinite space. Then what ?]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>5</wp:comment_id>
<wp:comment_author><![CDATA[pau]]></wp:comment_author>
<wp:comment_author_email></wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>82.228.182.110</wp:comment_author_IP>
<wp:comment_date>2006-02-18 16:20:57</wp:comment_date>
<wp:comment_date_gmt>2006-02-18 15:20:57</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Uniform pdf on the real numbers don't exist, so a uniform prior on a real variable can not be assumed. However, the likelihood can be considered instead of the posterior to obtain the desired effect.
Beside this, one must be aware that uniform priors are not non-informative. If someone knows what non-informative means, please explain.

And concerning the post title, as long as the likelihood is a well-behaved bounded function, it will have a max.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>6</wp:comment_id>
<wp:comment_author><![CDATA[pau]]></wp:comment_author>
<wp:comment_author_email></wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>82.228.182.110</wp:comment_author_IP>
<wp:comment_date>2006-02-18 16:23:18</wp:comment_date>
<wp:comment_date_gmt>2006-02-18 15:23:18</wp:comment_date_gmt>
<wp:comment_content><![CDATA[i hope you are happy. you have 1 reader ;-)]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>7</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-04 10:05:29</wp:comment_date>
<wp:comment_date_gmt>2006-03-04 09:05:29</wp:comment_date_gmt>
<wp:comment_content><![CDATA[I think ML is better viewed from Empirical Risk Minimization standpoint than Bayesian. IE, you are trying to fit a model to data as hard as you can. If you are able to prove that your training error is close to test error (which you can do under some conditions, through Statistical Learning Theory), then minimizing training error (in this case, log-likelihood) also minimizes test error, and hence is the right thing to do.

As far as noninformative priors, Pau, you aren't the only one that thinks they don't exist -- http://www.isds.duke.edu/research/conferences/valencia/Dialogue.pdf]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>8</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-03-13 17:04:37</wp:comment_date>
<wp:comment_date_gmt>2006-03-13 16:04:37</wp:comment_date_gmt>
<wp:comment_content><![CDATA[This paper is really interresting and pleasant to read. Bernardo explains his "reference priors", that are non-subjective priors maximising the amount of information brought by data. He presents lots of good properties of them. But they are often improper. In this case, Bernardo advice that "one should not interpret them as probability distribution", but rather as a "technical device" to produce non-subjective, proper and "sensible" posteriors. Sensible means "of value for scientifique communication". More explainations are in the text.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>MAP is BAD</title>
<link>http://yamlb.wordpress.com/2006/02/15/map-is-bad/</link>
<pubDate>Wed, 15 Feb 2006 17:16:24 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Decision]]></category>

		<category domain="category" nicename="decision"><![CDATA[Decision]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/15/map-is-bad/</guid>
<description></description>
<content:encoded><![CDATA[Maximum A Posterori, is often used when people design bayesian based algorithms. Sometime it is good, sometime not. We have to understand why.

In the correct bayesian approach, the result of the inference is a probability distribution. The <strong>entire</strong> distribution <strong>is the</strong> solution. Of course, In a lot of concrete problems, a hole distribution cannot be used as an output, you need to have just an element. For instance, when you drive robots, the motors need to be fed with ONE value and not a distribution.

So in such cases, we have to choose one number (lets says that the distribution is on real numbers), who is the <strong>"best"</strong> representative of the entire distribution (which is a function, so which is an infinity of real numbers in term of information content).

There is absolutely no reason to think that the MAP is always the best. And I can give common situations where it's a bad idea to take the MAP. Example: if the posterior is the sum of a big gaussian centered on 0, and of a really sharp peak (a Dirac) centered on 100. If I have to choose a representative, I'd choose 0, which is not the MAP. Actually it depends on the problem, and one has to carefully think on which representative should be taken. Candidates are
<ul>
	<li>MAP</li>
	<li>mean (really bad if several big modes)</li>
	<li>draws(can lead to oscillations in the motor command)</li>	
       <li> max of the posterior "convolved" with a shifting windows (in order to smooth it)</li>
</ul>
For this last idea, there is a parameter, the window size, which depend on the application. This last idea isn't just a trick, I think that it's closely related to <strong>automatic Occam razor</strong>("marginalised likelihood"). Actually this is not a real convolution, but rather a discretisation. <strong>Discretisation is a complex problem, much more fundamental than it's usually thought.</strong>

Another clear example were MAP is BAD is the estimation of a gaussian mixture modelling a set of points. The MAP, with a flat prior, is clearly achieved when you put as many gaussians as points, each of them with zero std (diracs). 

Another way to simply explain why MAP is bad, is saying that MAP maximises the <strong>probability density</strong>. But the density itself has no meaning, this is not a "belief". In general, the density isn't a function, it's a distribution, in sens of Schwartz (Dirac...)

What is relevant is the <strong>probability</strong>, which is an integration of the density around a point. <em><strong>The real problem-dependant question is to decide the size of the area we integrate on.</strong></em>
That's why the marginalised likelihood makes sense, when you consider it as a posterior with uniform prior integrated on a subregion of the space. That's why there is

(Again, this follows a discussion with <a href="http://www.inrialpes.fr/movi/people/gargallo/">Pau,</a> lots of ideas are from him)]]></content:encoded>
<wp:post_id>25</wp:post_id>
<wp:post_date>2006-02-15 18:16:24</wp:post_date>
<wp:post_date_gmt>2006-02-15 17:16:24</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>map-is-bad</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>9</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-04 10:10:29</wp:comment_date>
<wp:comment_date_gmt>2006-03-04 09:10:29</wp:comment_date_gmt>
<wp:comment_content><![CDATA[There's another reason MAP is bad -- it's not invariant to reparametrization. In other words, if you reparametrize your model, and use MAP, it will lead to different inference decisions. So for applying MAP, the modeller needs to choose a "good" parametrization (however that's found), so it's one more place to make a bad modelling decision. I wrote up a small note showing this in a coin-tossing example -- http://web.engr.oregonstate.edu/~bulatov/research/reports/bad_map.ps]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>10</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-03-08 11:56:42</wp:comment_date>
<wp:comment_date_gmt>2006-03-08 10:56:42</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Thanks, that's a really clear example.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>11</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-04-02 10:21:41</wp:comment_date>
<wp:comment_date_gmt>2006-04-02 09:21:41</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Here's a related paper I just came across -- <a href="http://www-sop.inria.fr/ariana/personnel/Ian.Jermyn/publications/InvariantBayesianEstimationAnnStat.pdf" rel="nofollow">INVARIANT BAYESIAN ESTIMATION ON MANIFOLDS</a>.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>ML Slides in french</title>
<link>http://yamlb.wordpress.com/2006/02/15/ml-slides-in-french/</link>
<pubDate>Wed, 15 Feb 2006 17:35:41 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Uncategorized]]></category>

		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/15/ml-slides-in-french/</guid>
<description></description>
<content:encoded><![CDATA[Last year, I was running a reading group in our lab. Here are the <a href="http://emotion.inrialpes.fr/~dangauthier/MLWGStuff/">slides</a> in french.

Just one concerns bayesian stuff (mine, uhuhhu), and all are introductions to some sub-subjects of machine learning.]]></content:encoded>
<wp:post_id>26</wp:post_id>
<wp:post_date>2006-02-15 18:35:41</wp:post_date>
<wp:post_date_gmt>2006-02-15 17:35:41</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>ml-slides-in-french</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Bayesianism doesn&#8217;t justify Occam Razor</title>
<link>http://yamlb.wordpress.com/2006/02/15/bayesianism-doesnt-justify-occam-razor/</link>
<pubDate>Wed, 15 Feb 2006 18:08:50 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Decision]]></category>

		<category domain="category" nicename="decision"><![CDATA[Decision]]></category>

		<category><![CDATA[Priors]]></category>

		<category domain="category" nicename="priors"><![CDATA[Priors]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/15/bayesianism-doesnt-justify-occam-razor/</guid>
<description></description>
<content:encoded><![CDATA[One could say that someone is starting to be a bayesian when he understand what is Automatic Occam Razor. I won't introduce it, but have a look on slides or papers from Neal or Ghahramani to find out some explanations.

Let's take the example of a mixture of gaussian model for modelling the density of a sample of points. The parameter space is the cartesian product : A=<strong>R</strong>^2*<strong>R</strong>^4*<strong>R</strong>^6*...*<strong>R</strong>^2n*...*<strong>R</strong>^(2*infinity), because one has to chose the numb re of gaussians (K) in <strong>N</strong>, and for each one, to choose the mean (m) and standard deviation (s).

For the moment, let's choose and uniform (improper argh!) prior on this space A. The MAP is a point in <strong>R</strong>^(nb of data), but it's clearly not a good solution for us. Why ? 
<ul>
	<li>because it has no theoretical reason to be a good solution (see previous post)</li>
	<li>because this model is able to generalised (the same data set with one point slightly moved will give an almost zero probability to the model)</li>
	<li>because this model has as complex (in the sens of information weight) as the data set, then one can say we summarised nothing, we learnt nothing</li>
</ul>

Bayesian people can find a better model, even <strong>even with a flat prior on K</strong>, by, in a first step finding the optimal K, the most probable K, by using the marginalised likelihood (I don't like this name), ie by marginalising on each <strong>R</strong>^2K space.
After choosing that K (which is small thanks to automatic Occam razor, ie thanks to the marginalisation), one can just run EM to find the most probable (m,s) knowing K.

Remark: marginalising is not easy, the BIC is an approximation.

What I want to add is that the way to marginalise on (m,s) spaces is a little bit arbitrary, here it's intuitive to find K in the first step, but in a general problem, where we don't have a clear semantic on the parameters, other methods to subdivide the A space before marginalising can be good. In the mixture problem K controls the complexity of the model, but could we find problem where marginalising regarding a normal parameter can lead to a good solution ?
I think so, because, more than the convenient idea of Occam razor (which can be discussed...), this is working because we want very probable models, and not just with high density. We want models in an local area with a lot of probability mass. The difficult question is the size and the shape of the area.

That's why I think that congratulating our self for this "automatic Occam" is <strong>not</strong> really relevant. This feature appear in bayesian statistics just because we <em>choose to marginalise regarding a parameter controlling complexity</em>, but <strong>this is just a choice</strong>, and this is not a justification of the "simplicity is better" philosophy.
]]></content:encoded>
<wp:post_id>27</wp:post_id>
<wp:post_date>2006-02-15 19:08:50</wp:post_date>
<wp:post_date_gmt>2006-02-15 18:08:50</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>bayesianism-doesnt-justify-occam-razor</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Next posts</title>
<link>http://yamlb.wordpress.com/2006/02/16/next-posts/</link>
<pubDate>Thu, 16 Feb 2006 11:08:46 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/02/16/next-posts/</guid>
<description></description>
<content:encoded><![CDATA[What subjects for nexts posts? Ideas:
<ul>
	<li>Discretisation.</li>
	<li>What people from different commuties mean when they say they are "bayesian".</li>
	<li>What is bayesianism for me</li>
	<li>Belief propagation and Sudoku </li>	
        <li>List of Inference methods</li>
	<li>Meaning of uniform distribution on infinite space (improper prior)</li>
	<li>The need of a theory for probabilities on surface spaces</li>
</ul>

]]></content:encoded>
<wp:post_id>28</wp:post_id>
<wp:post_date>2006-02-16 12:08:46</wp:post_date>
<wp:post_date_gmt>2006-02-16 11:08:46</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>next-posts</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Questionable methods Awards !</title>
<link>http://yamlb.wordpress.com/2006/03/03/questionable-methods-awards/</link>
<pubDate>Fri, 03 Mar 2006 17:34:22 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/03/03/questionable-methods-awards/</guid>
<description></description>
<content:encoded><![CDATA[Some A.I. methods are questionable regarding several criteria, and here is a list of methods I don't like :
<ul>
<ol> <strong>1st</strong> : Fuzzy logic</ol>
<ol> <strong>2nd</strong> : Genetic algorithms</ol>
<ol> <strong>3rd</strong> : (classical) Neural nets</ol>
</ul>

They have been chosen regarding their :
<ul>
	<li>weak mathematical foundations (GA)</li>
	<li>questionable philosophical or common sens justification (fuzzy)</li>
	<li>abscence of generality (neural nets 
	</li><li>small range of successful applications (fuzzy, genetic programming)</li>
        <li>Hard tuning (GA)</li>
</ul>

More seriously, Konstantin Tretjakov produced <a href="http://ats.cs.ut.ee/u/kt/stuff/scholartrend/">nice scripts</a> showing the number of publications on a particular suibject during time. The idea is from Yaroslav Bulatov, more informations on his <a href="http://yaroslavvb.blogspot.com/">blog</a>. I present here some results:

<img src="http://emotion.inrialpes.fr/~dangauthier/images/fuzzylogic-1980-2005-machinelearning.png" alt="Fuzzy" />
<img src="http://emotion.inrialpes.fr/~dangauthier/images/geneticalgorithm-1980-2005-machinelearning.png" alt="genetic" />
<img src="http://emotion.inrialpes.fr/~dangauthier/images/neuralnetwork1980-2005-machinelearning.png" alt="neural" />
<img src="http://emotion.inrialpes.fr/~dangauthier/images/bayesian-1980-2005-machinelearning.png" alt="bayes" />
<img src="http://emotion.inrialpes.fr/~dangauthier/images/bp.png" alt="belief" />
<img src="http://emotion.inrialpes.fr/~dangauthier/images/sex.png" alt="sex" />
<img src="http://emotion.inrialpes.fr/~dangauthier/images/exp.png" alt="expert" />
<img src="http://emotion.inrialpes.fr/~dangauthier/images/mc.png" alt="montecarlo" />
<img src="http://emotion.inrialpes.fr/~dangauthier/images/svm-1980-2005-machinelearning.png" alt="svm" />
Please notice that these are conditional probabilities based on counting, and that year 2006 is not really relevant.]]></content:encoded>
<wp:post_id>29</wp:post_id>
<wp:post_date>2006-03-03 18:34:22</wp:post_date>
<wp:post_date_gmt>2006-03-03 17:34:22</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>questionable-methods-awards</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>12</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-03-08 13:45:26</wp:comment_date>
<wp:comment_date_gmt>2006-03-08 12:45:26</wp:comment_date_gmt>
<wp:comment_content><![CDATA[I edited this post because it was gratuitously polemical.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>13</wp:comment_id>
<wp:comment_author><![CDATA[Will Dwinnell]]></wp:comment_author>
<wp:comment_author_email>predictr@bellatlantic.net</wp:comment_author_email>
<wp:comment_author_url>http://will.dwinnell.com</wp:comment_author_url>
<wp:comment_author_IP>141.151.11.105</wp:comment_author_IP>
<wp:comment_date>2007-02-09 11:42:15</wp:comment_date>
<wp:comment_date_gmt>2007-02-09 10:42:15</wp:comment_date_gmt>
<wp:comment_content><![CDATA[This post seems very vague.  Can you explain, for instance, what precisely you mean when you write that genetic algorithms have "weak mathematical foundations"?]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>14</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>82.228.182.110</wp:comment_author_IP>
<wp:comment_date>2007-02-09 14:35:08</wp:comment_date>
<wp:comment_date_gmt>2007-02-09 13:35:08</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Oh, sorry, do not feel offended, this post is actually very vague and yes, claims are not justified. A lot of brilliant people work and achieve stuff with GA. However, we have to remember that :
* GA is not magic, (when a pb is hard, it is hard)
* GA are extremely far from what happen in real evolution. A metaphor is not a proof.
* GA is just a global optimization algorithm when an oracle gives points value of the fitness landscape. There are other optimization processes.

Do we have some proofs of convergence to an (even local) optimum? Not really, there some work (schema theory) about over-simplified situations (infinite nb of individuals, no crossovers). The general case is difficult because GA performance is *highly* dependent on :
- the fitness landscape smoothness/topology
- the string representation of individuals (binary...)
- the choice of operators (mutation/crossover/selection)

Indeed there is the "No Free Lunch theorem" which basically assert that you can't expect an algorithm to work for all problems, if you don't bring some domain knowledge (roughly a "prior").

With GA this knowledge has to be put in the definition of *good* representation/operators, adapted to what you know about the shape of the fitness. And you would be surprised to see how many people try to use GA as a magical "black box".

I just want to say that one must be careful with GA. As a global optimization heuristic, they can help, but it's not enough to say that you have build an artificial intelligence algorithm.

Pierre]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Inference methods</title>
<link>http://yamlb.wordpress.com/2006/03/03/inference-methods/</link>
<pubDate>Fri, 03 Mar 2006 18:40:33 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Inference]]></category>

		<category domain="category" nicename="inference"><![CDATA[Inference]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/03/03/inference-methods/</guid>
<description></description>
<content:encoded><![CDATA[List of inference methods I've heard about:
<ul>
	<li> <em>Exact and numerical</em> : reorganizing sum and products for exact inference on discrete variables (juction tree)</li>
	<li><em> Exact and analytical</em> : using conjugate priors, posteriors can be analytically derived.</li>
	<li> <em>Appoximation by sampling</em> : All kind of sampling including the family of Markov chains Monte Carlo methods (Gibbs sampling seems to be often used)</li>
	<li> <em>Approximation with variational approach</em>: mean field, message passsing, belief propagation, (expectation propagation?). They consit in minimising a "distance" between the searched distribution with a member of  a class of candidate distributions</li>
	<li><em>Approximations based on posterior modes</em> : Laplace approximation, EM-related algorithms</li>

</ul>


]]></content:encoded>
<wp:post_id>30</wp:post_id>
<wp:post_date>2006-03-03 19:40:33</wp:post_date>
<wp:post_date_gmt>2006-03-03 18:40:33</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>inference-methods</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>15</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-07 09:10:51</wp:comment_date>
<wp:comment_date_gmt>2006-03-07 08:10:51</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Maybe you've seen this before, Wainwright shows a very cool way to derive message passing algorithms through variational approach http://research.microsoft.com/uai2004/Slides/Wainwright.pdf]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>16</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-11-20 21:18:36</wp:comment_date>
<wp:comment_date_gmt>2006-11-20 20:18:36</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Some corrections/precisions:

Belief propagation is exact on trees (de-localization of computations), expectation propagation and variational method are similar in the sense they are both minimizing a KL divergence between the true posterior p and an approximate one (q). But EP tries to (locally) minimize KL(p,q) and variational minimizes KL(q,p). This makes a difference, because the first leads to a "conservative", "secure", "inclusive" approximation, and variational results are too confident.
Mean Field is a simple variational method.
Also EM can be understood in the framework of variational approximation.
Loopy Belief propagation can be seen as a special case of EP with a fully factorized approximation family.
There are also all those "free energy minimization" methods, Kikuchi approximation, Generalised  belief propagation and Tree reweightseted BP stuff. 

Ouf !!!]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Sudoku and Belief Propagation</title>
<link>http://yamlb.wordpress.com/2006/03/06/sudoku-and-belief-propagation/</link>
<pubDate>Mon, 06 Mar 2006 19:25:27 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Inference]]></category>

		<category domain="category" nicename="inference"><![CDATA[Inference]]></category>

		<category><![CDATA[Modeling]]></category>

		<category domain="category" nicename="modeling"><![CDATA[Modeling]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/03/06/sudoku-and-belief-propagation/</guid>
<description></description>
<content:encoded><![CDATA[Last month, with <a href="http://www.inrialpes.fr/movi/people/gargallo/">Pau</a>, we tried to solve a fashionable problem with a fashionable algorithm.

We implemented loopy B.P. on the sudoku graph : one node for each bin of 
the sudoku grid, edges linking bins of the same row, column or region, 
and compatibility matrix = ones(9)-eye(9).

Our conclusions are :
<ul>
        <li>as it was predictable, this straightforward implementation isn't a 
good algorithm for solving Sudoku.</li>
	<li>LBP converged to the good solution for simple sudokus,</li>
	<li>but failed to converge or converged to a wrong solution for difficult 
sudokus.</li>
        <li>maybe a "dual" approach considering numbers as nodes could give better 
results, but we think this problem is intrinsically difficult for BP, because 
of the "you-can't-be-like-me" rule.</li>
</ul>

We recently heard of other people working on these ideas, with other results.

]]></content:encoded>
<wp:post_id>31</wp:post_id>
<wp:post_date>2006-03-06 20:25:27</wp:post_date>
<wp:post_date_gmt>2006-03-06 19:25:27</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>sudoku-and-belief-propagation</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>17</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-07 09:13:25</wp:comment_date>
<wp:comment_date_gmt>2006-03-07 08:13:25</wp:comment_date_gmt>
<wp:comment_content><![CDATA[might find this interesting http://blogs.zdnet.com/emergingtech/?p=177]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>18</wp:comment_id>
<wp:comment_author><![CDATA[Christopher]]></wp:comment_author>
<wp:comment_author_email>tay@inrialpes.fr</wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>194.199.21.108</wp:comment_author_IP>
<wp:comment_date>2006-03-07 10:45:21</wp:comment_date>
<wp:comment_date_gmt>2006-03-07 09:45:21</wp:comment_date_gmt>
<wp:comment_content><![CDATA[With regards to using BP to solve sudoku, I was wondering whether probabilistic methods are suitable for solving a "correct/wrong" problem. In the case of sudoku, the solution is either wrong or correct. Sudoku is of a different nature to problems such as the travelling salesman problem where a probabilistic methods gives approximately good solutions.

Returning to sudoku, it might be possible to put constraints on the system either explicitly or implicitly by changing the topological representation of the nodes.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>19</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-03-08 11:28:47</wp:comment_date>
<wp:comment_date_gmt>2006-03-08 10:28:47</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Yaroslav: Thanks for all your comments. For this particular one, I had a look on the link, it's quite difficult to understand what is exactly the "difference-map algorithm" they use. However, there exists good deterministic algorithms for sudoku (cf wikipedia), we think that BP has no chance to outperform them, but as its success in loopy random fields are not well understood, we just tried it. If it was successful (but it wasn't), it would have been an elegant solution in the sense that we didn't had to think too much about the pb, we just translated the rules in a graph+matrix and let BP run.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>20</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-03-08 15:41:15</wp:comment_date>
<wp:comment_date_gmt>2006-03-08 14:41:15</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Chritopher: I disagree when you say that sudoku and TSP are of different nature. I think that there exists approximate solutions to sukoku, some better that others (nb of wrong numbers, nb of unsatisfied constraints). And then it's possible to see suduku as an optimisation problem, and to apply variational approaches.
For your 2nd remark, I totaly agree. Adding redundant contraints or devising a better topology could better the convergence of BP.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>21</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-10 04:22:42</wp:comment_date>
<wp:comment_date_gmt>2006-03-10 03:22:42</wp:comment_date_gmt>
<wp:comment_content><![CDATA[The reporter made it sound like a monumental discovery, but I couldn't make much sense of the PNAS paper either, thought perhaps somebody else could]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>22</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-10 04:26:01</wp:comment_date>
<wp:comment_date_gmt>2006-03-10 03:26:01</wp:comment_date_gmt>
<wp:comment_content><![CDATA[A general comment, if you are using WordPress, it would be cool to have a sidebar with all the recent comments (like on John L.'s blog)]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>23</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-03-10 10:41:42</wp:comment_date>
<wp:comment_date_gmt>2006-03-10 09:41:42</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Done. ;-)]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>24</wp:comment_id>
<wp:comment_author><![CDATA[paper shredders]]></wp:comment_author>
<wp:comment_author_email>charmspray@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://www.shredderwarehouse.com</wp:comment_author_url>
<wp:comment_author_IP>202.163.102.160</wp:comment_author_IP>
<wp:comment_date>2006-12-21 06:44:15</wp:comment_date>
<wp:comment_date_gmt>2006-12-21 05:44:15</wp:comment_date_gmt>
<wp:comment_content><![CDATA[The presence of short cycles in the graph creates biases so that not every puzzle is solved by this method. However, all puzzles are at least partly solved by this method]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Bayesian Brain Again?</title>
<link>http://yamlb.wordpress.com/2006/03/13/bayesian-brain-again/</link>
<pubDate>Mon, 13 Mar 2006 00:25:33 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Modeling]]></category>

		<category domain="category" nicename="modeling"><![CDATA[Modeling]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/03/13/bayesian-brain-again/</guid>
<description></description>
<content:encoded><![CDATA[While writing next post about the meaning of "bayesian" across communities, I decided to make a post just for psychophysics. I'm not qualified, but it seems that a "bayesian rational" agent is a good model for certain human behaviors.

For instance, it seems that humans are optimally combining multiple sensory cues, "optimality" taken in the sense of "bayes optimal". O. Bousquet discuss this <a href="http://ml.typepad.com/machine_learning_thoughts/2006/01/bayesian_brain.html">here</a>. Optimality aside, psycho-physicists are proposing that some human behaviors could be explained by considering a probabilistic model+loss function, and with some subjective priors like "velocities are small".

But, it's rather difficult to measure "human belief" functions,  for that they uses frequencies in repeated (and biased) experiments, which isn't really what we want to measure. However finding probabilistic models is a promising and exiting research field, even if it'll be hard to prove something like "the brain is performing bayesian inference" or "neurons are implementing belief propagation".

Evolutionary speaking, being rational is an advantage, and when you face uncertainty, probabilistic inference can be your friend. Then it's reasonable to think that we're performing approximate inference. 

To conclude, for neuroscientists, bayesian brain means performing inference on belief states, with subjective priors acquired during evolution/education.
]]></content:encoded>
<wp:post_id>32</wp:post_id>
<wp:post_date>2006-03-13 01:25:33</wp:post_date>
<wp:post_date_gmt>2006-03-13 00:25:33</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>bayesian-brain-again</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>25</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-03-16 16:47:40</wp:comment_date>
<wp:comment_date_gmt>2006-03-16 15:47:40</wp:comment_date_gmt>
<wp:comment_content><![CDATA[I'd like to add that, in our daily life, that's not obvious that we always take "rational" decisions. Especially on high level problems, more complex than sensory integration. There are sevral articles in  <a href="http://www.amazon.com/gp/product/0521284147/102-8505437-7435317?v=glance&#38;#38;n=283155" title="Kahneman" rel="nofollow">Kahneman (1982)</a> showing that we violate the rules of probability calculs in important ways.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Which &#8220;Bayesian&#8221; are you ?</title>
<link>http://yamlb.wordpress.com/2006/03/14/which-bayesian-are-you/</link>
<pubDate>Tue, 14 Mar 2006 11:12:46 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Modeling]]></category>

		<category domain="category" nicename="modeling"><![CDATA[Modeling]]></category>

		<category><![CDATA[Priors]]></category>

		<category domain="category" nicename="priors"><![CDATA[Priors]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/03/14/which-bayesian-are-you/</guid>
<description></description>
<content:encoded><![CDATA[Nowadays, it seems to be very fashionable to be <strong>a bayesian</strong>, and it would certainly be interesting to compare <em>P(accepted) </em>and <em>P(accepted|"bayesian blabla" in the title)</em> for the same article across different computer science-related communities.
But what exactly people mean when they apply to themselves the adjective "bayesian"? Here is a proposal, based on the (small) knowledge I have on a (small) sample of communities:
<ul>
	<li><em>I'm a bayesian if </em> I use the word "<strong>probability</strong>".</li>
	<li><em>I'm a bayesian if </em> I use the word "<strong>conditional </strong>probability".</li>
	<li><em>I'm a bayesian if </em> I use <strong>Bayes' rule</strong>.</li>
	<li><em>I'm a bayesian if </em> I use Bayes' rule in order to make <strong>inference</strong>.</li>
	<li><em>I'm a bayesian if </em> I think that probabilities represent <strong>states of knowledge.</strong></li>
        <li><em>I'm a bayesian if </em> I think that probabilities represent states of knowledge and I <strong>also  </strong> consider my parameters as random variables. </li>
	<li><em>I'm a bayesian if </em> I think that probabilities represent states of knowledge and I use <strong>priors </strong>(no MaxLikelihood). </li>
	<li><em>I'm a bayesian if </em> I think that probabilities represent states of knowledge and I use priors and I use <strong>priors on priors</strong> (hierarchical models).</li>
	<li><em>I'm a bayesian if </em> I think that probabilities represent states of knowledge and I use <strong>subjective </strong>priors.</li>
        <li><em>I'm a bayesian if </em> I think that probabilities represent states of knowledge and I use priors and I never use <a href="http://emotion.inrialpes.fr/~dangauthier/blog/2006/02/15/map-is-bad/">MAP</a>.</li>
</ul>

Theses categories are not ordered neither incompatible for some of them. Other ideas ?]]></content:encoded>
<wp:post_id>33</wp:post_id>
<wp:post_date>2006-03-14 12:12:46</wp:post_date>
<wp:post_date_gmt>2006-03-14 11:12:46</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>which-bayesian-are-you</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>26</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-14 22:50:38</wp:comment_date>
<wp:comment_date_gmt>2006-03-14 21:50:38</wp:comment_date_gmt>
<wp:comment_content><![CDATA[I think the key aspect that distinguishes Bayesians from other probabilistic camps is the idea that our prior knowledge is specified in the form of real-valued function over belief states. By contrast, for MaxEnt and minimax camps, prior knowledge is specified as constraints on the space of models

MAP may sound more like MDL, but it's sometimes motivated as a crude approximation to the full BMA integral.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>27</wp:comment_id>
<wp:comment_author><![CDATA[hal]]></wp:comment_author>
<wp:comment_author_email>me-nospam-@hal3.name</wp:comment_author_email>
<wp:comment_author_url>http://hal3.name/</wp:comment_author_url>
<wp:comment_author_IP>24.130.77.159</wp:comment_author_IP>
<wp:comment_date>2006-03-16 16:37:42</wp:comment_date>
<wp:comment_date_gmt>2006-03-16 15:37:42</wp:comment_date_gmt>
<wp:comment_content><![CDATA[In my mind, anything but the first five on your list (and maybe the 5th, but that's borderline) are reasonable (this agrees with Yaroslav's comment above).  Note that you don't really need to <i>believe</i> these things: you just need to behave as if you do.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>28</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-03-20 15:23:07</wp:comment_date>
<wp:comment_date_gmt>2006-03-20 14:23:07</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Hal: Do you want to stress a kind of insincerity among the "bayesian" community ? ;-) .
I understood your comment as "to be a bayesian, just behave as if you believe that porbabilities are...", was it the meaning of your comment ?]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>29</wp:comment_id>
<wp:comment_author><![CDATA[hal]]></wp:comment_author>
<wp:comment_author_email>me-nospam-@hal3.name</wp:comment_author_email>
<wp:comment_author_url>http://hal3.name/</wp:comment_author_url>
<wp:comment_author_IP>24.127.65.231</wp:comment_author_IP>
<wp:comment_date>2006-03-21 02:31:37</wp:comment_date>
<wp:comment_date_gmt>2006-03-21 01:31:37</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Pierre: I didn't mean to imply Bayesians are insincere.  But I don't think it's bad to use something you don't believe (entirely) in either.  When people posit a model for a problem, almost by definition they don't believe this model is true.  But this model can still be useful.  For instance, several people I know use GPs to solve certain classification problems, but I have no idea if any of them actually believe in the Bayesian paradigm.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>30</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-03-22 04:25:12</wp:comment_date>
<wp:comment_date_gmt>2006-03-22 03:25:12</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Bayesians who don't believe in their priors=Objective Bayesians. Like Wasserman and Bernardo. So here, the view is that Bayesian approach provides a good averaging technique, rather than being "the most logical thing to do"

I find it surprising that anybody actually believe their priors represent their knowledge. Even for something as simple as coin tossing, Bayesian prior is a function over reals, so it specifies a degree of belief for uncountably many points. Do we really believe that at each point, there's a corresponding "internal belief state" that matches exactly what the prior specifies? 

There are other ways in which priors we pick may not correspond to our beliefs, ie, there's non-invariance to reparametrization and Bertrand's paradox.

Another example of the strangeness -- one tosses a coin, and they have a belief that coin is *not* fair. So they want the inference procedure to favour the tails, and so they guess that this knowledge can be represented by using Bayesian approach with Beta(0,0) prior since Beta(0,0) concentrates the mass towards the tails. But using Bayesian Model Averaging with this prior is equivalent to Maximum Likelihood which does not favour the tails, so the guess was wrong. 

(*Technically Beta(0,0) is improper, but you can do BMA in the limit as a,b-&#38;gt;0)]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>31</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>82.228.182.110</wp:comment_author_IP>
<wp:comment_date>2006-03-22 20:31:43</wp:comment_date>
<wp:comment_date_gmt>2006-03-22 19:31:43</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Bernardo proposes some "reference priors", ie priors of reference, priors you can use when you don't know how to define a subjective prior. Those priors are defined in the way that they are the most "weak", the  "less informative" priors you can find. Then I gess that he can still think that "bayesian is the most logical thing to do".

I'm also confused by the meaning of "probability density", of "belief density" for the continuous case. As I remember, Jaynes advice to stay on the finite cases, and then to check if passing to the limit is valid.

For the non-invariance discussion, I agree that picking a prior without enough care often lead to priors not matching our belief. The problem is even more difficult in high dimensional, or topologically strange spaces. How to check a prior ?]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>32</wp:comment_id>
<wp:comment_author><![CDATA[Julien]]></wp:comment_author>
<wp:comment_author_email>Julien.Diard@upmf-grenoble.fr</wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>195.221.43.31</wp:comment_author_IP>
<wp:comment_date>2006-04-25 12:21:09</wp:comment_date>
<wp:comment_date_gmt>2006-04-25 11:21:09</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Just a little anecdote. I gave part of a talk at a "Bayesian Cognition" workshop last fall. It was a three day workshop, with speakers from communities as varied as robotics, neuroscience, experimental psychology. 

My talk was on the first day. The first speaker of the second day was a philosopher, and he introduced his speech by a summary of how much the first day had been un-Bayesian. He listed the talks of the previous day, mentionning at each time how the subject treated or model proposed was not Bayesian. As for my talk, he said something along those lines, as far as memory serves :

"The first model presented, by Julien D., concerned human navigation. After his talk, I went to him and asked him what exactly was Bayesian about his model. He answered, quite honestly I think, that his model was not *really* Bayesian. He said : "You know, for us engineers, Bayesian roughly means "using probabilities""".

I'm actually rather proud of the "quite honestly I think" in the above comment. :D

In other words, and to go back to the topic, defining "Bayesian" is difficult. It gets much easier when you specify which community you're referring to. In robotics nowaydays, for instance, Bayesian often refers to work using probabilities (as opposed to, say, fuzzy logic). In cognitive neuroscience, it's more demanding, because people there are more used to the distinction between MLE and Bayesian estimations. 

As for the "picky" vocabulary and distinction in the above commentaries between Objective Bayesians, subjective freqentists, etc, I learned them today, reading this site. As long as the definition is clear, I don't mind using the same word for yet another meaning. 

You just have ot be sure the one you're talking to uses roughly the same meaning. :D

My 2c
Julien]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>33</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-04-25 13:44:13</wp:comment_date>
<wp:comment_date_gmt>2006-04-25 12:44:13</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Hi Julien, nice to read you here.
Concerning your anecdote, it was actually the inspiration of this post ;-)
I totaly agree with you, this is a problem of vocabulary.
But even in robotics engeenering, beeing "more" bayesian could (and do) serve.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>test</title>
<link>http://yamlb.wordpress.com/?p=34</link>
<pubDate>Fri, 17 Mar 2006 10:47:39 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=34</guid>
<description></description>
<content:encoded><![CDATA[hello how aree you]]></content:encoded>
<wp:post_id>34</wp:post_id>
<wp:post_date>2006-03-17 11:47:39</wp:post_date>
<wp:post_date_gmt>2006-03-17 10:47:39</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Statistical evolution</title>
<link>http://yamlb.wordpress.com/2006/03/23/statistical-evolution/</link>
<pubDate>Thu, 23 Mar 2006 16:04:52 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Modeling]]></category>

		<category domain="category" nicename="modeling"><![CDATA[Modeling]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/03/23/statistical-evolution/</guid>
<description></description>
<content:encoded><![CDATA[I found this picture on <a href="http://quasar.as.utexas.edu/stat295.html">Bill Jeffery's' statistical course web page.</a> Jeffery strongly prefers the bayesian approach to statistics.

<a href="http://quasar.as.utexas.edu/courses/stat295.2005/evobayes.jpg"><img src="http://emotion.inrialpes.fr/~dangauthier/images/evobayes2.jpg" alt="Bayes evolution" /></a>

To explain this comic, let's quote Pr. Jefferys:
<blockquote>The basic difference between Bayesians and frequentists is this: 
Bayesians condition on the data actually observed, and consider the 
probability distribution on the hypotheses; they believe it reasonable 
to put probability distributions on hypotheses and they behave 
accordingly. Frequentists condition on a hypothesis of choice and 
consider the probability distribution on the data, whether observed or 
not; they do not think it reasonable to put probability distributions on 
hypotheses (in their opinion, one hypothesis is true, the rest are 
false, even if we do not know which is the case), and they behave 
accordingly.</blockquote>

Now the question is: What is thought by "Homo sapiens" ? I understand all others except him. Is it just to make something funny, to respect the original cartoon with five characters? Or can you assign a meaning on reasoning on the joint distribution?]]></content:encoded>
<wp:post_id>35</wp:post_id>
<wp:post_date>2006-03-23 17:04:52</wp:post_date>
<wp:post_date_gmt>2006-03-23 16:04:52</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>statistical-evolution</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>health capital</title>
<link>http://yamlb.wordpress.com/?p=36</link>
<pubDate>Thu, 30 Mar 2006 17:15:49 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=36</guid>
<description></description>
<content:encoded><![CDATA[Le capital sante se mesurerait en nb d annee a vivre. On pourrait en faire une applet qui soit parlante.
Inference bayesienne derriere. Il faut des donnees. Quelles sont les features importantes ?]]></content:encoded>
<wp:post_id>36</wp:post_id>
<wp:post_date>2006-03-30 18:15:49</wp:post_date>
<wp:post_date_gmt>2006-03-30 17:15:49</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Advice for the Young Scientist</title>
<link>http://yamlb.wordpress.com/2006/04/03/advice-for-the-young-scientist/</link>
<pubDate>Mon, 03 Apr 2006 17:37:28 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

		<category><![CDATA[Work]]></category>

		<category domain="category" nicename="work"><![CDATA[Work]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/03/advice-for-the-young-scientist/</guid>
<description></description>
<content:encoded><![CDATA[To change from bayesian statistics, I found this page dealing with some <a href="http://math.ucr.edu/home/baez/advice.html">Advice for the Young Scientist</a>, written by John Baez in 2003.
Citation:
<blockquote>
<ul>
	<li>Keep your soul</li>
	<li>Go to the most prestigious school.</li>
	<li>Work with the best possible advisor</li>
	<li>Publish.</li>
	<li>Go to conferences.</li>
	<li>Give Good Talks</li>
</ul></blockquote>



]]></content:encoded>
<wp:post_id>37</wp:post_id>
<wp:post_date>2006-04-03 18:37:28</wp:post_date>
<wp:post_date_gmt>2006-04-03 17:37:28</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>advice-for-the-young-scientist</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>34</wp:comment_id>
<wp:comment_author><![CDATA[Ronan]]></wp:comment_author>
<wp:comment_author_email></wp:comment_author_email>
<wp:comment_author_url>http://blog.bayesien.org</wp:comment_author_url>
<wp:comment_author_IP>82.236.189.224</wp:comment_author_IP>
<wp:comment_date>2006-04-03 21:00:57</wp:comment_date>
<wp:comment_date_gmt>2006-04-03 20:00:57</wp:comment_date_gmt>
<wp:comment_content><![CDATA[No kidding... He forgot this: brush your teeth regularly.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Exam mark generative model</title>
<link>http://yamlb.wordpress.com/?p=38</link>
<pubDate>Mon, 10 Apr 2006 20:57:21 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/?p=38</guid>
<description></description>
<content:encoded><![CDATA[Today, I corrected exams of my students, and as the class exams are divided into 3 teachers, we have to make the fusion of our marks. usually  we just compute the 3 average marks and add the required nu opf po,its to each copies in ordrer to have the sameaverage mak.

But tjos is unfair, what is the best procedure ? certainly the one who refelecst the stuednt skill, what they learn

une ltterature extensive but

evaluate skill from marks]]></content:encoded>
<wp:post_id>38</wp:post_id>
<wp:post_date>2006-04-10 21:57:21</wp:post_date>
<wp:post_date_gmt>2006-04-10 20:57:21</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name></wp:post_name>
<wp:status>draft</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Frequencist/bayesian differences in french</title>
<link>http://yamlb.wordpress.com/2006/04/12/frequencistbayesian-differences-in-french/</link>
<pubDate>Wed, 12 Apr 2006 19:09:48 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Priors]]></category>

		<category domain="category" nicename="priors"><![CDATA[Priors]]></category>

		<category><![CDATA[philosophy]]></category>

		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/12/frequencistbayesian-differences-in-french/</guid>
<description></description>
<content:encoded><![CDATA[This is my reply following some posts on the <a href="http://math-et-physique.over-blog.com/article-2295006-6.html#anchorComment">Mathéphysique french blog</a> of Fabien Besnard. It shows that the difference between bayesian and frequencist interpretation of probabilities <strong>is definitely leading to</strong> differences on pratical case.
<em>The problem is the following: You are in front of an urn with an unkown number of labelled balls. You pick the ball number 2. How many balls are in the urn ? Is 10 more likely than 1000 ?
The same problem arise when you try to guess the number of Taxis in an unknown city after seeing the number of one of them</em> (see <a href="http://research.microsoft.com/%7Eminka/papers/uniform.html">Minka paper</a>).
In short, frequencists proposes several estimators, bayesians propose to use the Pareto as conjugate distribution of the uniform one. With prior knowledge (K!=0), bayesian posterior (or even point representation of the posterior) permits better prediction, better generalisation, and avoids overfitting.
<blockquote>Il y a beaucoup de chose dans votre message, et beaucoup de chose que j ai envie de commenter, mais je vais essayer de me focaliser sur quelques points. D'abord il peut être intéressant de faire la distinction entre théorie des probabilités (modelé mathématique rigoureux et non (peu) discuté) et son application au vrai monde, c'est a dire une théorie physique, qui s'appelle les statistiques. En théorie des probabilités (axiomatique de Kolmogorov, théorie de la mesure) on a pas besoin d'interprétation et il n y a donc pas débat freq/bay. On a des variables aléatoires avec des propriétés mathématiques, comme la loi des grands nombres. Cette dernière est un théorème issu des axiomes, et ne confirme en rien une interprétation ou l autre. Par contre, des que j'étudie le monde, le débat émerge, car selon ma position, je ne ferai pas les mêmes calculs sur mes données. Vous parlez d'estimateurs, un concept typiquement frequenciste qui n a pas vraiment de sens en bayesien. C est la dépendance des estimateurs sur des donnes non observées qui pose pb.
<strong>L'interprétation des probas a pour conséquence une approche statistique différente, c est comme ça,</strong> le débat n est pas la, si vous n étés pas d accord,et bien vous vous trompez ;-). Un bayesien ne calculera jamais un "estimateur sans biais" ni un intervalle de confiance et un frequenciste ne calculera jamais une densité a posteriori, ni de proba sur les paramètres d un modèle.

Pour ce qui est de votre exemple, on peut comparer les deux approches, mais faut aller rigoureusement jusqu au bout pour voir que ça ne donnera pas les même résultats.

<em>Le PB est : moi, expérimentateur, j ai une urne devant moi dont je ne vois pas la taille, mais de laquelle j ai tiré un 2. Et je veux dire si il est plus "probable" qu' il y ait 10 ou 1000 boules.</em>

Pour les deux approches, soit k le nb de boules au total, alors k appartient a IN. <strong>Notre espace des possibles est IN.</strong>

<strong>* Approche frequenciste:</strong> Déjà on a un PB, car comme il n y a qu' une seule et unique urne, k est fixe et non de nature aléatoire, k a une certaine valeur, a la limite et par convention on peut dire que c est une var. aléatoire constante, de proba 0 partout et 1 dans sa vraie valeur. Alors la question de savoir si il est plus probable k=10 ou k=1000 n a pas de sens du tout. La question qui a un sens est de construire un estimateur de cette valeur k a partir des données. Un estimateur k^ (le chapeau est sur le k) est une heuristique ayant de "bonnes" propriétés, y en a autant qu' on peut avoir de définition de "bonne". Il est courant de prendre l estimateur de max. de vraisemblance k^=ArgMax_k P(Donne; si le parm. est k). Ici on a 1 donnée, et il est raisonnable de modéliser P(D;k)=unif(1..k). Alors k^=ArgMax (1/k si k=&gt;2, 0 sinon)=2. L'estimateur de maxVraiss est k^=2, c est comme ça, bien qu il soit peut intuitif. Cet estimateur k^=D n'as pas la bonne propriété d'être sans biais car E[k^]=E[D]=k/2 != k. On peut alors proposer comme autre estimateur k^^ = 2D, qui est sans biais. Bref, il a plein d d'estimateurs, en choisir un a qq chose de subjectif, non ? Et on a pas répondu a la question, car on dit des fois que k=4, et des fois que k=2, ... Mais aucune procédure frequenciste ne permet de trancher objectivement entre 10 et 1000.

<strong>* Approche bayesienne:</strong> Ce qui manque a l approche frequenciste, c est de pouvoir dire qu on aime mieux les petites urnes. Sans ça, on ne peut rien dire de plus que k&gt;=2. (Cette impossibilité de dire plus de choses se traduit par l impossibilité de préférer un estimateur a un autre).
Ici on a un PB réel, un PB physique qu on veut résoudre. Et on sait des choses sur le monde physique, comme par exemple que l urne ne peut pas être arbitrairement grande. C'est ce que le philosophe pensait intuitivement, il apportait des connaissances autres que le strict énoncé. Dans le vrai monde, les petits nb de boules sont plus fréquent que les grands, donc un observateur a une plus grande CROYANCE que l urne soit petite que grande. Cette croyance depend de plein de choses, elles est subjective, elle depend de l experience passée de l'observateur (jamais vu d urne de la taille de la terre) mais aussi de conseils qu on a recu (ma mere ma dit qu'il n existait pas plus de 1000 boules sur la terre), et aussi de modele physique (cette urne de 1m3 ne peut contenir plus de 20000 de boules)... Cette Croyance est difficile a colecter, a formaliser ( c est un GROS point faible du bayesien), mais en admettant qu on l ait formalisee sous forme d un fonction de IN entre 0 et 1, alors on va faire les raisonnement "optimaux".
Dans l'exemple, comme la proba est une mesure de croyance, je peux mettre des probas sur les evenements k=1, k=2 ..., je peux considerer k comme une variable aleatoire (le terme aleatoire est historique, car en bayesien je peux parler de proba meme si il n y a pas d'exp.aleatoire, si tout est determiné. D'ailleurs il est diffcile de definir le hasard, de savoir si le monde est deterministe ou pas. Tout ceci est flou, en tout cas pour moi). Bref, j ai une urne, j ai tiré un D=2 et je me pose la question parfaitement definie P(k=10 | D=2) &gt;?&lt; P(k=10000 | D=2). Pour cela je peut faire le rapport et je dois utliser Bayes : P(10|2)/P(1000|2) =  P(2|10)/ P(2|1000)  *  P(10)/ P(1000). On a toujours la vraissemblance  P(D|k)=unif(1..k) ce qui donne au premier terme la valeur 100, le pb est le 2eme, le prior.
Pour choisir ce prior P(k), y a plusieurs ecoles, et une biblio de these a faire. Il faut integrer la contrainte de normalisation (Summ P(k) = 1) et aussi notre a priori de decroissance, voir de nullité apres un seuil. C'est un boulot de modelisation de ses connaissances. D'autre part gens se posent des pb d'ecoles comme : et si on sait rien ? Et si on sait juste que E[k]=1500 ? et si on veut que notre prior influence le moins possible ? et si on veut que le resultat ne depende pas d'un certaine transofrmation sur k ? et si on veut que les calculs soient faciles ?
Une option courrante est de pendre un prior parametrique conjugé, c est a dire facile a manipuler et suffisamment souple pour representer notre croyance. Ici on veut surtout la decroissance en k, d'une force reglable f. Le prior qui va bien est celui de Pareto de parametre (1,f). Sans renter dans les details, on arrive au final sur P(10)/ P(1000)=(100)^(f+1). Donc dans le rapport des posteriors on voit que les donnee donnee 100 fois plus de confiance en la valeur 10, et que notre a priori lui donne une confiance (100)^(f+1) fois plus grande. Tant que f&gt;0 (ie decroissance), 10 est bien plus "probable" que 1000. Une anlyse mathematique plus fine montre qu au pire si on prend un "prior plat" (mm si il n est pas normalisé), on a un rapport de 100^2. Dans ce cas, le mode du posterior est egale a l estimateur de maxVrai frequenciste, et sa mediane a l estimateur sans biais. Mais ce ne justifie pas l approche frequenciste, c'est une "coincidence".

<strong>Donc, en definite les frequencistes, ne voulant pas utiliser de prior sur k, ne peuvent pas dire plus que ce qu ils disen</strong>t. Les bayesiens galerent pour formaliser leurs priors pour un pb donné, mais une fois que c est fait, ces infos permettent de dire plus de choses sur le pb, forcement. Il est vrai qu'en freq, on aurrait pu considerer un double exp aleat. imaginaire, on aurrait pu considerer que l urne elle meme etait tiree et donc que k etait une V.A. Mais dans ce cas il faut definir cette population imaginaire d urnes, ce qui revinent exactement a donner un prior sur k, et dans un cadre vachement plus boiteux que le bayesien (exp. imaginaire mal definie peut entrainer des paradoxes (Bertarnd)).

Le frequencisme c est mal quand on l applique sans comprendre, c est mal quand on refuse de prendre des infos qu on a pourtant, c est mal quand on interprete mal ses resultats (pvalues), c'est mal car faut toujours d inventer de nouvelles techniques (estimateurs) pour chaque pb, <strong>mais surtout c est mal quand on croit que c est un methode objective</strong>. On ne peut pas avoir des conclusions sans faire d'hypothese, avoir des priors ca fait chier, mais c'est necessaire. Sinon, on se voile la face, on cache ses hypotheses dans des procédures obscures.

En résumé :
<em>&gt; il n'y a pas de différence calculatoire entre bayésien et fréquentiste,</em>
Si ! il y en a. On n'utilise pas les meme informations, on ne calcule pas les meme quantités, on ne fait pas les memes predictions. Et meme si dans certains cas on fait numeriquement le meme calcul, ca ne veut absolument pas dire qu'on en tire les meme conclusions.</blockquote>]]></content:encoded>
<wp:post_id>39</wp:post_id>
<wp:post_date>2006-04-12 20:09:48</wp:post_date>
<wp:post_date_gmt>2006-04-12 19:09:48</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>frequencistbayesian-differences-in-french</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>35</wp:comment_id>
<wp:comment_author><![CDATA[Fabien Besnard]]></wp:comment_author>
<wp:comment_author_email>Fabien.Besnard@wanadoo.fr</wp:comment_author_email>
<wp:comment_author_url>http://math-et-physique.over-blog.com</wp:comment_author_url>
<wp:comment_author_IP>82.121.164.173</wp:comment_author_IP>
<wp:comment_date>2006-04-13 09:46:24</wp:comment_date>
<wp:comment_date_gmt>2006-04-13 08:46:24</wp:comment_date_gmt>
<wp:comment_content><![CDATA[&#38;gt;Un bayesien ne calculera jamais un “estimateur &#38;gt;sans biais” ni un intervalle de confiance et un &#38;gt;frequenciste ne calculera jamais une densité a &#38;gt;posteriori, ni de proba sur les paramètres d un &#38;gt;modèle.

C'est vraiment très manichéen non ? On a l'impression que selon toi on est bayésien par définition quand on applique certaines méthodes. Mais cela revient à changer la définition de ce terme qui pour moi renvoi avant tout à une interpréation la théorie des probabilités. Si ce que vous dites était vrai cela reviendrait à remettre en cause l'unité de la science. On aurait deux camps qui ne pourraient pas se parler, ni comparer leurs résultats. Je soutiens, mais peut-être me trompe-je, qu'au contraire on peut avoir une attitude positiviste, s'en remettant aux prédictions de la théorie et à ses vérifications expérimentales. Ce qui ne signifie pas que les interprétations n'aient aucun intérêt, j'ai d'ailleurs écrit que celles-ci inspiraient une méthode plutôt qu'une autre.
Je lis la suite.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>36</wp:comment_id>
<wp:comment_author><![CDATA[Fabien Besnard]]></wp:comment_author>
<wp:comment_author_email>Fabien.Besnard@wanadoo.fr</wp:comment_author_email>
<wp:comment_author_url>http://math-et-physique.over-blog.com</wp:comment_author_url>
<wp:comment_author_IP>82.121.164.173</wp:comment_author_IP>
<wp:comment_date>2006-04-13 10:11:15</wp:comment_date>
<wp:comment_date_gmt>2006-04-13 09:11:15</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Bien, j'ai lu et je n'ai rien vu de différent que ce que j'ai déjà écrit sur mon blog. Vous confondez, c'est probablement une déformation professionnelle, la méthodologie et l'interprétation. Il est clair que les deux méthodologies que vous décrivez sont inspirées chacune par un "camp", mais je peux parfaitement interpréter vos "prior" bayésiennes d'un point de vue fréquentiste. Et in fine c'est ce qu'il faudra faire si on veut comparer nos résultats avec une expérience. En effet, vous avez parfaitement résumé le point de vue fréquentiste, a priori on ne peut rien dire sur k. C'est un point de vue philosophique si l'on veut, mais il correspond parfaitement à la situation idéalisée telle qu'elle est décrite : il manque une information, et dans le monde étheré des mathématiques on ne peut rien faire sans cette information. Ce que vous faite dire ensuite au baysésien c'est qu'on peut deviner d'une certaine manière cette information manquante. Mais vous changer subrepticement de monde, vous passer d'un énoncé idéalisé à un énoncé sur le monde réel. Et tout fréquentiste qu'on soit, on peut parfaitement admettre que dans le monde réel k a été choisi d'une certaine manière. Si on ne connait pas la loi de k (car si on la connait on passe alors à un exo de proba, et exit la dichotomie méthodologique) on va faire appel à différentes heuristiques qui ont peut-être été inspirées par les bayésiens mais que je peux parfaitement reprendre à mon compte de façon pragmatique, quel que soit mon point de vue sur la question (j'ai aussi le droit d'être agnostique ou de manger à tous les rateliers). In fine, le fréquentiste dira quelle que chose du genre "en faisant l'hypothèse supplémentaire que la loi de k est ... alors P(k=10|D=2)=..." et le baysésien "en utilisant la prior bidule on trouve P(k=10|D=2)=...", ce qui revient exactement au même !]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>37</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-04-16 03:00:13</wp:comment_date>
<wp:comment_date_gmt>2006-04-16 02:00:13</wp:comment_date_gmt>
<wp:comment_content><![CDATA[In principle, nothing stops a frequentist from using an estimator that is identical to the Bayes estimator with Pareto Prior, (motivated by practical performance). Similarly, there's nothing restricting Bayesians to Pareto prior. 

I think the key difference between Bayesian and frequentist is how they communicate the inductive bias underlying their estimators. Bayesians use "Bayesian prior", whereas frequentists use labels like sufficiency, minimaxity, or simply "good performance on datasets A,B,C".

So the Bayesian vs. frequentist debate IMHO is about which language to use to encode our prior knowledge. The language affects the way we think, so that's why I think in practice Bayesians and frequentist end up coming up with different estimating methods.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>38</wp:comment_id>
<wp:comment_author><![CDATA[Mich blum from michigan]]></wp:comment_author>
<wp:comment_author_email>michblum@umich.edu</wp:comment_author_email>
<wp:comment_author_url>http://sitemaker.umich.edu/michael.blum</wp:comment_author_url>
<wp:comment_author_IP>141.211.38.9</wp:comment_author_IP>
<wp:comment_date>2006-07-06 21:51:01</wp:comment_date>
<wp:comment_date_gmt>2006-07-06 20:51:01</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Hye Pierre,

I am glad to see that your blog is still active. Your toy exemple (the taxi problem from Minka) is nice but is not of practical purposes in the real world. I think that the big difference between frequentist and bayesian approaches comes from the operational algorithm used to adress a specific problem. Basically frequentists use maximization algorithms (such as EM) whereas bayesians use algorithms that perform integration (MCMC, Gibbs...). Let me give you a concrete exemple that comes from population genetics. Populations geneticists want to perform clustering of individuals based on genetic data. But more than that, they want to know the admixture proportions for each individual. For instance, assume that there are 2 populations (France-Italy), they want to know for each individual, the proportion of the genome that is Italian and the proportion of the genome that is French. So assuming K (usually around 5) populations and n (between 50 and 1000) individuals, there are at least n^K parameters to infer which is enormous. For sure, the likelihood landscape is complicated and an optimization algorithm will be stuck in a local optima. Be bayesian, use MCMC and to know the answer, just wait 48 hours looking at the screen of your computer.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>39</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>213.199.128.156</wp:comment_author_IP>
<wp:comment_date>2006-07-10 13:31:32</wp:comment_date>
<wp:comment_date_gmt>2006-07-10 12:31:32</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Hye Mich blum from michigan !

I just partly agree with your statement. That's true that frequentists estimates are often point estimations, but, in a way, one could see confidence intervals as a kind of "integration" of estimations.

On the other hand, bayesian posteriors (possibly approximated with MCMC) are real "integration", but a lot of methods rely on MAP, which is a point estimation.

In this view, bayesianism is better because you <strong>know</strong> that the best answer is the <strong>hole</strong> posterior distribution, even is you sometimes take a point representation of this function for computational reasons.

Anyway, by representing a infinite dimensional (the posterior, or the likelihood) object with just a point, you surely loose a lot of its information. That's why this decision should be done the latter possible in your inference process.

By the way, for youy example, I think you mean K^n instead of n^K for the number of parameters.

Cheers,
Pierre dang from Cambridge.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Interpretation of probability</title>
<link>http://yamlb.wordpress.com/2006/04/12/interpretation-of-probability/</link>
<pubDate>Wed, 12 Apr 2006 19:38:39 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[philosophy]]></category>

		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/12/interpretation-of-probability/</guid>
<description></description>
<content:encoded><![CDATA[To understand the debate between frequencists and bayesian, I read about the philosophy of the probability concept. The problem is deeper than just the common debate. After reading the very interesting article from <a href="http://plato.stanford.edu/entries/probability-interpret/">Plato Encyclopedia</a>, I wrote <a href="http://emotion.inrialpes.fr/%7Edangauthier/TEK_NOTES/interpretationProba/dangauthier06interProba.pdf">
</a><a target="_blank" title="pdf document" href="http://emotion.inrialpes.fr/bibemotion/2006/Dan06/dangauthier06interpretProba.pdf"> this</a> not-so-short note.

According to this article, there are several main interpretations:
<ul>
	<li><strong>Classical probabilities</strong> - based on the principle of indifference.</li>
	<li><strong>Logical probabilities</strong> - notion of "degree of implication" in a formal language.</li>
	<li><strong>Frequency interpretation</strong> - limiting relative frequencies in an hypothetical infinite sequence of trials.</li>
	<li><strong>Propensity</strong> - quality of the physical word. It represents an intrinsec tendency to behave in a certain way.</li>
	<li><strong>Subjective interpretation </strong>- degree of belief of a suitable agent.</li>
</ul>
<strong>Conclusion:</strong>
Probability theory and its statistical aspects play a special role in science, mainly because they are a necessary tool to understand and interact with the complex real world. When using probabilities to speak about experiments, one has to provide a meaning, a physical interpretation of this concept. Depending on this interpretation, he should use the theory in different ways, for instance by assigning probabilities to different kind of objects.
Despite the fact that all interpretations face some difficulties, bayesianism proposes a paradox-free, intuitive, consistent and rational methods for inference in a variety of practical cases. Moreover, bayesianism is a promising approach to understand quantum mechanics, rational decision making, psychology, and more generally, it has been presented as an extension of Aristotle's logic]]></content:encoded>
<wp:post_id>40</wp:post_id>
<wp:post_date>2006-04-12 20:38:39</wp:post_date>
<wp:post_date_gmt>2006-04-12 19:38:39</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>interpretation-of-probability</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>40</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>213.199.128.156</wp:comment_author_IP>
<wp:comment_date>2006-07-10 14:26:22</wp:comment_date>
<wp:comment_date_gmt>2006-07-10 13:26:22</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Related material:

Probability, Causality and the Empirical
World: A Bayes–de Finetti–Popper–Borel Synthesis
A. P. Dawid

http://www.ucl.ac.uk/Stats/research/Resrprts/psfiles/STS088.pdf]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Valencia 8 Meeting in June</title>
<link>http://yamlb.wordpress.com/2006/04/13/valencia-8-meeting-in-june/</link>
<pubDate>Thu, 13 Apr 2006 11:11:50 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Events]]></category>

		<category domain="category" nicename="events"><![CDATA[Events]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/13/valencia-8-meeting-in-june/</guid>
<description></description>
<content:encoded><![CDATA[<a href="http://www.bayesian.org/">ISBA</a>  <a href="http://www.uv.es/valenciameeting">8 world meeting</a> on Bayesian Statistics - June 1st-June 6Th 2006 - Will be organised in Alicante, Spain by  Professor J. Bernardo. Here is the 4 Mo <a href="http://emotion.inrialpes.fr/~dangauthier//images/V8Poster-2.pdf">poster (pdf)</a>.]]></content:encoded>
<wp:post_id>41</wp:post_id>
<wp:post_date>2006-04-13 12:11:50</wp:post_date>
<wp:post_date_gmt>2006-04-13 11:11:50</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>valencia-8-meeting-in-june</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Ultimate machine learning algorithm ?</title>
<link>http://yamlb.wordpress.com/2006/04/19/ultimate-machine-learning-algorithm/</link>
<pubDate>Wed, 19 Apr 2006 14:02:13 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Priors]]></category>

		<category domain="category" nicename="priors"><![CDATA[Priors]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/19/ultimate-machine-learning-algorithm/</guid>
<description></description>
<content:encoded><![CDATA[J. Langford on his <a title="blog" target="_blank" href="http://hunch.net/?p=176">blog</a> said:
<blockquote>"I am probably extreme as a machine learning person in wanting highly (or fully) automated methods"</blockquote>
After reading another post on <a href="http://ml.typepad.com/machine_learning_thoughts/2006/03/freakonomics.html">O.Bousquet blog</a>, it seems to me that the main difficulty for having "fully automated methods" is to automate the collection and the encoding of preliminary knowledge.

Whatever you call it, "prior probability", inductive bias, loss function, etc, this knowledge you (a human) have about a problem is fundamental for good results. Relying just on, say, a 2 hours movie of data for learning what is a human face in a noisy video seems to me impossible. For instance, for the problem of face tracking in images, a *huge* prior knowledge is coded in the shape and functionality of our body (eye, brain...) and a "smaller part" was acquired during life.

Then if we want "fully automated methods" for this kind of difficult real world problems, do we have to simulate those million years of evolution ? Does a "fully automated methods" need to contain all the knowledge for all the problems ? It seems difficult and recalls past temptatives to design a general problem solver, able to solve all problems...

In my opinion, taking into account this prior knowledge is the key idea, and one advantage of the bayesian approach is to say this explicitely. (even if deriving a prior function is far from easy!)

[EDIT] What's funny is that O. Bousquet has just made another <a target="_blank" href="http://ml.typepad.com/machine_learning_thoughts/2006/04/extracting_info.html">post</a> saying extracting knowledge from experts is <em>as important as</em> extracting it from data. Indeed when focussing to a particular class of problem, the preliminary knowledge is in experts minds.]]></content:encoded>
<wp:post_id>42</wp:post_id>
<wp:post_date>2006-04-19 15:02:13</wp:post_date>
<wp:post_date_gmt>2006-04-19 14:02:13</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>ultimate-machine-learning-algorithm</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>41</wp:comment_id>
<wp:comment_author><![CDATA[Olivier Bousquet]]></wp:comment_author>
<wp:comment_author_email>obousquet@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://obousquet.googlepages.com</wp:comment_author_url>
<wp:comment_author_IP>84.103.102.141</wp:comment_author_IP>
<wp:comment_date>2006-04-24 17:58:02</wp:comment_date>
<wp:comment_date_gmt>2006-04-24 16:58:02</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Hi Pierre,

Interestingly you saw exactly where I was heading to!
I agree hundred percent with you: the key is in the prior knowledge.
And the Bayesian approach makes this prior knowledge somewhat explicit. I say 'somewhat' because the prior distribution usually does not completely reflects the prior knowledge, for many different reasons (the knowledge cannot be easily expressed as a probability distribution, using a Gaussian approximation makes things easier, the knowledge is not explicit and thus a standard prior distribution is used...).
So now comes the question: what should we spend our efforts for? designing better algorithms or providing new ways to formalize and incorporate knowledge into algorithms?
And for me the answer is clear (especially because designing better algorithms does not make much sense except in a very restricted and properly defined framework).]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>3 debates about machine learning</title>
<link>http://yamlb.wordpress.com/2006/04/24/3-debates-about-machine-learning/</link>
<pubDate>Mon, 24 Apr 2006 15:49:44 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[philosophy]]></category>

		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/24/3-debates-about-machine-learning/</guid>
<description></description>
<content:encoded><![CDATA[<div style="text-align:center;"><a href="http://emotion.inrialpes.fr/~dangauthier/images/vapnik.jpg"><img width="296" height="219" alt="vapnik" src="http://emotion.inrialpes.fr/~dangauthier/images/vapnik.jpg" /></a></div>
Generally speaking machine learning research aims at designing machine or agents that learns from experience, that performs better days after days. It involves the capacity of memorising information and also the capacity of generalising it in order to predict what decision to take in front of a new situation.In this scope, probabilities are often used by researchers, mainly for 2 different purposes:

<strong>1 -</strong> Analysing the performances of learning algorithms, (ex: how often it takes good decisions on a bentchmark set)

<strong>2 - </strong>Or we can use probabilities and probabilty rules as the way the agent represents its knowledge on the world ("probabilty as logic")
<ol />Those two applications are compatible, and both imply that probability is an important notion for machine learning. But the use of probability raises some debates,a t different levels. Here are 3 of them:
<ol>
	<li><strong>Vapnik or Bayes ?
</strong>Over simplifing, Vapnik learning theory aims at minimizing the probability of making bad decision in the future. But it doen't recommand a special "technology" for the algorithm itself. The most common incarnation of this theory are SVM, which tries to maximise a margin, ie to maximise a generalisation abiltity, but which rely upon hard bondaries.
On the other hand bayesian learning systems are using probabilities as the core of knowledge representation.<strong>
</strong></li>
	<li><strong>Frequencies or degree of belief </strong>?
See previous posts. Is it better to interpret probability as an agent degree of belief, or as the limit of frequency in reapeted trials ? The bayesian point of vue, involving an agent, seems to be practical for machine learning.</li>
	<li><strong>Subjective or objective bayesianism</strong> ?
This is a debate among bayesians. For them, choosing a prior is difficult problem. A lot of methods have been proposed and this is the subject of a huge litterature. But a question divides the community: <em><strong>W</strong><strong>ill two agents with the same information choose the same prior ?</strong> </em>Objective bayesians think that it's the case, that there exist a best unik prior for each problem/knowledge.
On the contrary, subjectivists are saying that the choice of a prior is a matter of convention, it's upon the agent to choose a prior in the set of priors compatible with its knowledge.
This debate espacially arise when the prior has to be "non informative". Almost everybody agree that "knowing nothing", "non informativeness" don't make any sense. Subjectivists assert that one can only provide <em>"reference",</em> <em>"defaults"</em> priors which can help, and that no one can define a <em>"best"</em> prior.</li>
</ol>
Personally, at present day, I think that those debates are relevant, that those approches are different, even if there are some links. Due to my reading, I'd prefer Bayes over Vapnik, beliefs over frequencies and subjectivity over objectivity.]]></content:encoded>
<wp:post_id>43</wp:post_id>
<wp:post_date>2006-04-24 16:49:44</wp:post_date>
<wp:post_date_gmt>2006-04-24 15:49:44</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>3-debates-about-machine-learning</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Presentation</title>
<link>http://yamlb.wordpress.com/2006/04/25/presentation/</link>
<pubDate>Tue, 25 Apr 2006 09:31:17 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/25/presentation/</guid>
<description></description>
<content:encoded><![CDATA[ppt presentation]]></content:encoded>
<wp:post_id>44</wp:post_id>
<wp:post_date>2006-04-25 10:31:17</wp:post_date>
<wp:post_date_gmt>2006-04-25 09:31:17</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>presentation</wp:post_name>
<wp:status>attachment</wp:status>
<wp:post_parent>29</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:postmeta>
<wp:meta_key>_wp_attached_file</wp:meta_key>
<wp:meta_value>/var/www/emotion/people/dangauthier/blog/wp-content/uploads/2006/04/dangauthierTeamMeetingApril06withoutPictures.ppt</wp:meta_value>
</wp:postmeta>
<wp:postmeta>
<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
<wp:meta_value>s:6:"a:0:{}";</wp:meta_value>
</wp:postmeta>
	</item>
<item>
<title>presentation</title>
<link>http://yamlb.wordpress.com/2006/04/25/presentation-2/</link>
<pubDate>Tue, 25 Apr 2006 09:42:01 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/25/presentation-2/</guid>
<description></description>
<content:encoded><![CDATA[]]></content:encoded>
<wp:post_id>45</wp:post_id>
<wp:post_date>2006-04-25 10:42:01</wp:post_date>
<wp:post_date_gmt>2006-04-25 09:42:01</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>presentation-2</wp:post_name>
<wp:status>attachment</wp:status>
<wp:post_parent>29</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:postmeta>
<wp:meta_key>_wp_attached_file</wp:meta_key>
<wp:meta_value>/var/www/emotion/people/dangauthier/blog/wp-content/uploads/2006/04/dangauthierTeamMeetingApril06.pdf</wp:meta_value>
</wp:postmeta>
<wp:postmeta>
<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
<wp:meta_value>s:6:"a:0:{}";</wp:meta_value>
</wp:postmeta>
	</item>
<item>
<title>A team meeting presentation</title>
<link>http://yamlb.wordpress.com/2006/04/25/a-team-meeting-presentation/</link>
<pubDate>Tue, 25 Apr 2006 09:44:32 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/25/a-team-meeting-presentation/</guid>
<description></description>
<content:encoded><![CDATA[Here are some slides of a <a id="p30" href="http://emotion.inrialpes.fr/~dangauthier/blog/wp-content/uploads/2006/04/dangauthierTeamMeetingApril06.pdf">presentation</a> I made this morning, introducing bayesian learning. I have also copied/pasted some material from this blog (so nothing is really new).

Outline:
<ul>
	<li>Machine learning</li>
	<li>Philosophy of probability</li>
	<li>Bayesian Learning</li>
	<li>3 debates in machine learning</li>
	<li>Entropy maximization</li>
</ul>]]></content:encoded>
<wp:post_id>46</wp:post_id>
<wp:post_date>2006-04-25 10:44:32</wp:post_date>
<wp:post_date_gmt>2006-04-25 09:44:32</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>a-team-meeting-presentation</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>42</wp:comment_id>
<wp:comment_author><![CDATA[paresh]]></wp:comment_author>
<wp:comment_author_email>paresh.jain@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://research.iiit.ac.in/~paresh/cgi-bin/blog/blosxom.cgi</wp:comment_author_url>
<wp:comment_author_IP>202.65.145.4</wp:comment_author_IP>
<wp:comment_date>2006-04-25 13:34:04</wp:comment_date>
<wp:comment_date_gmt>2006-04-25 12:34:04</wp:comment_date_gmt>
<wp:comment_content><![CDATA[i think u've missed a '/' after dangauthier in the link to the presentation above.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>43</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-04-25 13:36:48</wp:comment_date>
<wp:comment_date_gmt>2006-04-25 12:36:48</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Yes, this is fixed now.
Thank you]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Maximum entropy and bayesian updating</title>
<link>http://yamlb.wordpress.com/2006/04/26/maximum-entropy-and-bayesian-updating/</link>
<pubDate>Wed, 26 Apr 2006 13:38:46 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/04/26/maximum-entropy-and-bayesian-updating/</guid>
<description></description>
<content:encoded><![CDATA[When I was young, I was thinking that there were one "best" way for belief updating (Bayes). I was also thinking that the problem with that was the definition of priors, and that, in certain cincunstances, the Maximum entropy principle was helpful.

In short I was thinking that "bayesian belief updating" and "maximum entropy" were two <strong>othogonal</strong> principles. But it appear that they are not, and that they can even be in conflict !

Example (from <a target="_blank" title="The Selection of Prior Distributions by Formal Rules" href="http://links.jstor.org/sici?sici=0162-1459(199609)91%3A435%3C1343%3ATSOPDB%3E2.0.CO%3B2-U">Kass 1996</a>); consider a Die (6 sides), consider prior knowledge <strong>E</strong>[X]=3.5.

Maximum entropy leads to P(X)= (1/6, 1/6, 1/6, 1/6, 1/6, 1/6).

Now consider a new piece of evidence A="X is an odd number"
<ul>
	<li>Bayesian posterior <em>P(X|A)= P(A|X) P(X) = (1/3, 0, 1/3, 0, 1/3, 0)</em>.</li>
	<li>But MaxEnt with the constraints <em><strong>E</strong>[X]=3.5</em> and<em> <strong>E</strong>[Indicator function of A]=1</em> leads to <em>(.22, 0, .32, 0, .47, 0) !!</em>  (note that <em><strong>E</strong>[Indicator function of A]=P(A)</em>)</li>
</ul>
Indeed, for MaxEnt, because there is no more '6', big numbers must be more probable to ensure an average of 3.5. For bayesian updating, P(X|A) doesn't have to have a 3.5 expectation. P(X) and P(X|a) are different distributions.
Conclusion ? MaxEnt and bayesian updating are two different principle leading to different belief distributions. Am I right ?]]></content:encoded>
<wp:post_id>47</wp:post_id>
<wp:post_date>2006-04-26 14:38:46</wp:post_date>
<wp:post_date_gmt>2006-04-26 13:38:46</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>maximum-entropy-and-bayesian-updating</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>44</wp:comment_id>
<wp:comment_author><![CDATA[Aleks]]></wp:comment_author>
<wp:comment_author_email>a_jakulin@hotmail.com</wp:comment_author_email>
<wp:comment_author_url>http://kt.ijs.si/aleks</wp:comment_author_url>
<wp:comment_author_IP>128.59.109.231</wp:comment_author_IP>
<wp:comment_date>2006-04-26 18:33:08</wp:comment_date>
<wp:comment_date_gmt>2006-04-26 17:33:08</wp:comment_date_gmt>
<wp:comment_content><![CDATA[The notion of the MaxEnt "prior" (as it also appears) is totally different from the notion of the Bayesian prior.

Instead, E[X]=3.5 is a *constraint*. Similarly, "X is odd" is also a constraint. Where do these constraints come from? Well, in statistical inference (or machine learning) you'd need to infer them from the data. Or, if they are indeed prior constraints, you'd zero out those parts of the prior parameter space that violate those constraints.

But one can combine MaxEnt and Bayes in the same framework. I often like to express my distributions in terms of parameters that are effectively constraints (parameterization-by-constraints). One would use MaxEnt to obtain the distribution given those constraints, but this is often difficult.

See <a rel="nofollow" href="http://dx.doi.org/10.1063/1.1835243">On The Relationship between Bayesian and Maximum Entropy Inference</a> by Cheeseman &#38; Stutz.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>45</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-04-27 14:32:59</wp:comment_date>
<wp:comment_date_gmt>2006-04-27 13:32:59</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Indeed, here event A is a constraint and not a piece of data as usually thought. Shimony (1973) showed that if we exented the space so that constraints are events and conditionalization is equivalent to MaxEnt, then, as you said, constraints will have prior probability one.

Thanks for the link, seems interesting, but the paper isn't freely available. Is there a "preprint" somewhere?]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>46</wp:comment_id>
<wp:comment_author><![CDATA[hal]]></wp:comment_author>
<wp:comment_author_email>me-nospam-@hal3.name</wp:comment_author_email>
<wp:comment_author_url>http://hal3.name/</wp:comment_author_url>
<wp:comment_author_IP>128.9.216.111</wp:comment_author_IP>
<wp:comment_date>2006-04-27 22:48:40</wp:comment_date>
<wp:comment_date_gmt>2006-04-27 21:48:40</wp:comment_date_gmt>
<wp:comment_content><![CDATA[<a href="http://mailgate.supereva.com/sci/sci.stat.consult/msg05289.html" rel="nofollow">This post</a> by Radford Neal might be of interest.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>47</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://yaroslavvb.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>24.22.21.212</wp:comment_author_IP>
<wp:comment_date>2006-04-28 09:25:55</wp:comment_date>
<wp:comment_date_gmt>2006-04-28 08:25:55</wp:comment_date_gmt>
<wp:comment_content><![CDATA[With MaxEnt there's a lot of freedom in choosing how data is converted into constraints. So while  subjective Bayesians are often criticized for arbitrary priors (by people like Wasserman), MaxEnt people are critized for their arbitrary data-&#38;gt;constraint conversion mechanism (by people like Radford Neal)

In Wasserman's example, I think there are better ways to set the constraints from data. For instance if we observed "dice landed on 1", his suggested method would contradict the previous constraint, so MaxEnt method would provide no solution. If we dropped the previous constraint, we'd get a solution, but a bad one.

We can motivate better MaxEnt constraints by the observation that observed counts and true counts will not be very far from each other. We can use tools from Statistical Learning Theory to figure out what is "far" and set constraints accordingly (range constraints on expected values). Miroslav Dudik did work in that direction and managed to get theoretical guarantees as well as good practical performance (http://www.cs.princeton.edu/~mdudik/PhillipsDuSc04.pdf)

But back to the topic, I agree that MaxEnt and Bayesian approaches can produce different inferences. Bayesian approach provides a unique way to incorporate data into your inference. But MaxEnt doesn't specify how to incorporate data, so people do all sorts of things -- constrain expected values of X, constrain expected values of Log X, constrain the median of the distribution, use inequality constraints, etc. 

(Uffink talks more about it http://www.citeulike.org/user/yaroslavvb/article/102807)

BTW, since you brought up Wasserman's priors paper, here's a paper from last MaxEnt conference that gives a cool justification of Jeffrey's prior http://web.engr.oregonstate.edu/~bulatov/papers/goyal-prior.pdf]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>48</wp:comment_id>
<wp:comment_author><![CDATA[David Corfield]]></wp:comment_author>
<wp:comment_author_email>david.corfield@tuebingen.mpg.de</wp:comment_author_email>
<wp:comment_author_url>http://www.dcorfield.pwp.blueyonder.co.uk/blog.html</wp:comment_author_url>
<wp:comment_author_IP>192.124.26.250</wp:comment_author_IP>
<wp:comment_date>2006-05-04 12:24:36</wp:comment_date>
<wp:comment_date_gmt>2006-05-04 11:24:36</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Following up one of the references from the last comment, I see there are two overlapping papers out this year:

Y. Altun and A. Smola, <a href="http://ttic.uchicago.edu/~altun/pubs/AltSmo-COLT06.pdf" rel="nofollow">Unifying Divergence Minimization and Statistical Inference via Convex Duality</a>.

M. Dudik and R. E. Schapire,  <a href="http://www.cs.princeton.edu/~mdudik/DudikSc06.pdf" rel="nofollow">Maximum entropy distribution estimation with generalized regularization</a>.

They seem to be general enough to allow a good part of statistical learning theory inside.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>49</wp:comment_id>
<wp:comment_author><![CDATA[Machine Learning (Theory) &#38;#187;]]></wp:comment_author>
<wp:comment_author_email></wp:comment_author_email>
<wp:comment_author_url>http://hunch.net/?p=209</wp:comment_author_url>
<wp:comment_author_IP>69.64.49.237</wp:comment_author_IP>
<wp:comment_date>2006-07-08 21:02:50</wp:comment_date>
<wp:comment_date_gmt>2006-07-08 20:02:50</wp:comment_date_gmt>
<wp:comment_content><![CDATA[[...] A few weeks ago I read this. David Blei and I spent some time thinking hard about this a few years back (thanks to Kary Myers for pointing us to it): [...] ]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type>pingback</wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>50</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>213.199.128.156</wp:comment_author_IP>
<wp:comment_date>2006-07-10 13:01:48</wp:comment_date>
<wp:comment_date_gmt>2006-07-10 12:01:48</wp:comment_date_gmt>
<wp:comment_content><![CDATA[This discussion is also continued on

http://hunch.net 

and 

http://www.dcorfield.pwp.blueyonder.co.uk/2006/07/conditionalization-as-i-projection.html]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Machine Learning Summer School</title>
<link>http://yamlb.wordpress.com/2006/05/03/machine-learning-summer-school/</link>
<pubDate>Wed, 03 May 2006 11:39:13 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/05/03/machine-learning-summer-school/</guid>
<description></description>
<content:encoded><![CDATA[<blockquote><strong>Invitation to the Machine Learning Summer School</strong>
July 24- August 4, 2006, Taipei, Taiwan

Quick Link: <a class="moz-txt-link-freetext" href="http://www.iis.sinica.edu.tw/MLSS2006">http://www.iis.sinica.edu.tw/MLSS2006</a>

MLSS Taipei 2006 is the 2006 version of the "Machine Learning
Summer School" series held previously in Canberra, Tubingen,
Berder and Chicago.

The two-week summer school program consists of about 40+ hours
of tutorial lectures from 7/24 to 8/4. The late afternoon
sessions will provide a chance for the participants
to discuss the latest research problems with the invited
speakers. This summer school will cover a broad range of
subjects from statistical learning theory to state of the art
applications. Please check the summer school Web site for the
topics covered and the list of invited speakers.

Poster (please help advertise the MLSS series in your school):
<a class="moz-txt-link-freetext" href="http://www.iis.sinica.edu.tw/MLSS2006/flyer.pdf">http://www.iis.sinica.edu.tw/MLSS2006/flyer.pdf</a></blockquote>]]></content:encoded>
<wp:post_id>48</wp:post_id>
<wp:post_date>2006-05-03 12:39:13</wp:post_date>
<wp:post_date_gmt>2006-05-03 11:39:13</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>machine-learning-summer-school</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>NIPS paper evaluation criteria</title>
<link>http://yamlb.wordpress.com/2006/05/22/nips-paper-evaluation-criteria/</link>
<pubDate>Mon, 22 May 2006 12:53:50 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/05/22/nips-paper-evaluation-criteria/</guid>
<description></description>
<content:encoded><![CDATA[<a title="nips" target="_blank" href="http://www.nips.cc/Conferences/2006/">NIPS</a> (Neural information processing systems) is a really relevant conference for the machine learning community. This year, it'll be held in Vancouver, Canada in december. The submission deadline is June, 9, and today <a title="hunch.net" target="_blank" href="http://hunch.net">J.Langford</a> posted a link to some <a title="criterai" href="http://research.microsoft.com/conferences/nips06/NIPS-evaluation.html">paper evaluation criteria</a> published by the conference organizers.]]></content:encoded>
<wp:post_id>49</wp:post_id>
<wp:post_date>2006-05-22 13:53:50</wp:post_date>
<wp:post_date_gmt>2006-05-22 12:53:50</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>nips-paper-evaluation-criteria</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>51</wp:comment_id>
<wp:comment_author><![CDATA[paper shredders]]></wp:comment_author>
<wp:comment_author_email>charmspray@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://www.shredderwarehouse.com</wp:comment_author_url>
<wp:comment_author_IP>202.163.102.160</wp:comment_author_IP>
<wp:comment_date>2006-11-16 05:16:48</wp:comment_date>
<wp:comment_date_gmt>2006-11-16 04:16:48</wp:comment_date_gmt>
<wp:comment_content><![CDATA[I found the document itself to be mostly common sense. I guess it’s nice to have the criteria I was already applying in my reviewing spelled out explicitly (assuming I am asked to review again), but I don’t feel like I’d do anything different after reading this document.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Exotic probabilities</title>
<link>http://yamlb.wordpress.com/2006/05/23/exotic-probabilities/</link>
<pubDate>Tue, 23 May 2006 15:45:30 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/05/23/exotic-probabilities/</guid>
<description></description>
<content:encoded><![CDATA[When we refer to the notion of probabilty, there are basically 2 main approches:
<ul>
	<li><strong>bayesian</strong>: based on Cox theorem: a probability is a positive real number representing a degree of belief.</li>
	<li><strong>frequentist</strong>: a probabilty is the limit of frequences in an infinite sequence of  random experiments.</li>
</ul>
In both cases, probabilities obey to Kolmogorov axiomatic, and in particular there are <strong>real numbers between 0 and 1</strong>.

The point of this post is : <em>Could we imagine defining probabilities with another set of numbers ?</em>

Does it make sense, and would it be useful, to define:
<ul>
	<li>probabilities greater than one ?</li>
	<li>negative probabilities ?</li>
	<li>complex probabilities ?</li>
	<li>quaternion probabilities ?</li>
	<li>...</li>
</ul>
In the fields of information theory (Shannon), game theory (von Neumann), classical statistics (Fisher), statistical physics (Boltzman-Maxwell-Gibbs) and statistical inference (Bayes-Jaynes), real number based probabilities have shown to be very productive. There is no reasons to change them, and to do without the useful underlying measure theory. On top of that the meaning of a, say, complex probabilty is counter intuitive and a bit confusing.

But there is <em>quantum mechanics</em>, where the wave function is a complex-valued function bringing all the information on a system. For instance the norm of this function is the probability density of the presence of a electron.

Instead of considering this norm, would it be possible to look at the complex number itself as a probabilty ? In this frameworks, would Feyman path integrals be a kind of marginalisation ? This would imply some constructive/destructive interferences when adding probabilities. What would be the interpretation of the phase part of a complex probabilty ?

I'm definitely not a quantum physicist, but I've found some theorical work on those ideas [<a title="youssef web site" href="http://physics.bu.edu/~youssef/">S.Youssef</a>]. This researcher argues that Cox requirements still holds when you consider measuring subjectives beliefs with complex numbers (or
quaternions for Dirac theory) on a distributive lattice of propositions (events). Then he argues that this simplifies a lot the interpretation of quantum mechanics and brings interessant results with Bell inequalities and renormalisation problems.

I can't judge the validty of those work, but I wonder why a so appealing theory isn't more studied.

Let's finish with a citation of a great physician,  Bill Jefferys:
<blockquote>There is no need to modify probability theory from any perspective
in order to do quantum mechanics. Bayesianism uses standard,
unmodified probability theory. Bayesian interpretations of QM use
standard, unmodified probability theory. [...] Such [complex, quaternion
probabilities] approaches are also not necessary and in my opinion
they confuse more than they illuminate.</blockquote>]]></content:encoded>
<wp:post_id>50</wp:post_id>
<wp:post_date>2006-05-23 16:45:30</wp:post_date>
<wp:post_date_gmt>2006-05-23 15:45:30</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>exotic-probabilities</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>52</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.200</wp:comment_author_IP>
<wp:comment_date>2006-05-24 09:58:26</wp:comment_date>
<wp:comment_date_gmt>2006-05-24 08:58:26</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Thanks for your comment.
Indeed, I prefer to see this as a kind of hierarchical modeling. If P(head)=q, if we don't know precisely q (q=0, q=1 or ???), we can consider q as a random variable itself, and put a probability density on it. Usually p(q)=Beta(a,b), the conjugate of the binomial. I guess that (a,b) are (more or less) the 2 numbers you are speaking about. IMHO, this is not linked with complex probability.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>53</wp:comment_id>
<wp:comment_author><![CDATA[John C.]]></wp:comment_author>
<wp:comment_author_email>john.benjamin.cassel@gmail.com</wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>192.17.82.219</wp:comment_author_IP>
<wp:comment_date>2006-05-23 19:29:11</wp:comment_date>
<wp:comment_date_gmt>2006-05-23 18:29:11</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Certain two-number formulations have been used with relatively widespread appeal in the uncertainty reasoning community, such as the belief and plausability approach of Dempster-Shafer.  Here, the intuition is that for an unflipped coin we have no belief that it will be heads, it could be perfectly flawed towards tails (belief=0), but it is also plausable that it could be perfectly biased towards heads (plausability=1).  For a fair coin, these would converge towards 0.5.  

But, admittedly, these meta-probabilistic frameworks really do little to address your question, since they are really just using multiple probabilities, as opposed to starting from the foundations.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>54</wp:comment_id>
<wp:comment_author><![CDATA[Julien]]></wp:comment_author>
<wp:comment_author_email>Julien.Diard@upmf-grenoble.fr</wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>195.221.43.31</wp:comment_author_IP>
<wp:comment_date>2006-06-09 13:04:52</wp:comment_date>
<wp:comment_date_gmt>2006-06-09 12:04:52</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Hi Pierre,

Quoting Jaynes' book, page 30 :

<blockquote>
[...] qualitative correspondence with common sense requires that <i>w(x)</i> be a positive continuous monotonic function. It may be either increasing or decreasing. If it is increasing, it must range from zero for impossibility up to one for certainty. It it is decreasing, it must range from (infinity) for impossibility down to one for certainty. [...]
However, these two possibilities of representation are not different in content. [...] Therefore, there will be no loss of generality if we now adopt the choice 0 w(x) convention. 
</blockquote>

If I remember correctly, Jaynes also cites the case that raising w(x) to any arbitrary power does not change much either : w(x), w^2(x), ..., w^m(x) are all possibilities for obtaining a formalism that is coherent with the initial desiderata. 

Anyway, in summary, yes, you can mess up with the probability system somewhat to have it look stranger than it already is. ^_^

As for using multi-dimensional representations of single degrees of belief... That goes out of the scope of Jaynes' theory (look up the first desiderata). As such, it is similar to "croiser les flux", it is baaad. ;-) But... why not? 

Cheers,
Julien]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>55</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>213.199.128.149</wp:comment_author_IP>
<wp:comment_date>2006-06-10 15:29:29</wp:comment_date>
<wp:comment_date_gmt>2006-06-10 14:29:29</wp:comment_date_gmt>
<wp:comment_content><![CDATA["Why not ?"
This is indeed the question.
Youssef says that it helps a lot for understanding QM, making some counter intuitive paradoxes disappear. And that Cox's requirements still holds.
Actually, QM *is* a probability theory on a space where operators don't commute.
Understanding QM is deeply related to understanding the notion of probabilty.
"Why not ?"
Maybe because 1-D probability densities are sufficient, according to Jefferys.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>MCMC_java_capture</title>
<link>http://yamlb.wordpress.com/2006/05/30/mcmc_java_capture/</link>
<pubDate>Tue, 30 May 2006 12:23:38 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/05/30/mcmc_java_capture/</guid>
<description></description>
<content:encoded><![CDATA[]]></content:encoded>
<wp:post_id>51</wp:post_id>
<wp:post_date>2006-05-30 13:23:38</wp:post_date>
<wp:post_date_gmt>2006-05-30 12:23:38</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>mcmc_java_capture</wp:post_name>
<wp:status>attachment</wp:status>
<wp:post_parent>38</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:postmeta>
<wp:meta_key>_wp_attached_file</wp:meta_key>
<wp:meta_value>/var/www/emotion/people/dangauthier/blog/wp-content/uploads/2006/05/MCMC.png</wp:meta_value>
</wp:postmeta>
<wp:postmeta>
<wp:meta_key>_wp_attachment_metadata</wp:meta_key>
<wp:meta_value>s:249:"a:5:{s:5:\"width\";i:598;s:6:\"height\";i:448;s:14:\"hwstring_small\";s:23:\"height=\'95\' width=\'128\'\";s:4:\"file\";s:76:\"/var/www/emotion/people/dangauthier/blog/wp-content/uploads/2006/05/MCMC.png\";s:5:\"thumb\";s:18:\"MCMC.thumbnail.png\";}";</wp:meta_value>
</wp:postmeta>
	</item>
<item>
<title>MCMC methods in a nutshell</title>
<link>http://yamlb.wordpress.com/2006/05/30/mcmc-methods-in-a-nutshell/</link>
<pubDate>Tue, 30 May 2006 12:34:25 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Inference]]></category>

		<category domain="category" nicename="inference"><![CDATA[Inference]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/05/30/mcmc-methods-in-a-nutshell/</guid>
<description></description>
<content:encoded><![CDATA[Laird Breyer presents a really nice<a target="_blank" href="http://www.lbreyer.com/classic.html"> Java applet</a> of Makov Chain Monte Carlo (MCMC) methods in action, with a short explanation of different proposal algorithms.
<a target="_blank" href="http://www.lbreyer.com/classic.html"><img width="357" height="268" alt="MCMC_java_capture" src="http://emotion.inrialpes.fr/%7Edangauthier/blog/wp-content/uploads/2006/05/MCMC.png" /></a>

He's also the author of a <a href="http://dbacl.sf.net">command line bayesian text classifier (dbacl)</a> , tunable for chess "playing" !!!]]></content:encoded>
<wp:post_id>52</wp:post_id>
<wp:post_date>2006-05-30 13:34:25</wp:post_date>
<wp:post_date_gmt>2006-05-30 12:34:25</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>mcmc-methods-in-a-nutshell</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>56</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>213.199.128.152</wp:comment_author_IP>
<wp:comment_date>2006-06-16 16:50:35</wp:comment_date>
<wp:comment_date_gmt>2006-06-16 15:50:35</wp:comment_date_gmt>
<wp:comment_content><![CDATA[I'm sorry but the address of this stuff changed, and I can't find it anymore on the web. Does someone knows an alternative address? (it was a really pedagogical applet). Does somebody has the email of the author?]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>57</wp:comment_id>
<wp:comment_author><![CDATA[Ronan]]></wp:comment_author>
<wp:comment_author_email>rlehy@free.fr</wp:comment_author_email>
<wp:comment_author_url>http://blog.bayesien.org</wp:comment_author_url>
<wp:comment_author_IP>194.199.21.32</wp:comment_author_IP>
<wp:comment_date>2006-07-10 11:04:50</wp:comment_date>
<wp:comment_date_gmt>2006-07-10 10:04:50</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Cool, it's back.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>On-line machine learning journals</title>
<link>http://yamlb.wordpress.com/2006/06/07/on-line-machine-learning-journals/</link>
<pubDate>Wed, 07 Jun 2006 16:47:13 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/06/07/on-line-machine-learning-journals/</guid>
<description></description>
<content:encoded><![CDATA[On-line and freely available papers are more cited than others, espetially in the computer science community. For instance, <a target="_blank" href="http://jmlr.csail.mit.edu/">JMLR</a> is online since 2000.

I discovered today another on line journal, <a target="_blank" title="JMLG" href="http://www.jmlg.org/">JMLG</a>. It would be shameful to miss its reading.]]></content:encoded>
<wp:post_id>53</wp:post_id>
<wp:post_date>2006-06-07 17:47:13</wp:post_date>
<wp:post_date_gmt>2006-06-07 16:47:13</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>on-line-machine-learning-journals</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Mathematical encyclopaedia</title>
<link>http://yamlb.wordpress.com/2006/06/15/mathematical-encyclopaedia/</link>
<pubDate>Thu, 15 Jun 2006 12:30:58 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/06/15/mathematical-encyclopaedia/</guid>
<description></description>
<content:encoded><![CDATA[Here are tree math encyclopaedia freely available:
<ul>
	<li><a href="http://mathworld.wolfram.com/">Wolfram MathWorld</a></li>
	<li><a href="http://en.wikipedia.org/wiki/Main_Page">Wikipedia</a></li>
	<li><a href="http://eom.springer.de/">Encyclopaedia of Mathematics</a> (less known, but with a lot of material, some seems outdated)</li>
</ul>
(source: <a target="_blank" title="gelman blog" href="http://www.stat.columbia.edu/~cook/movabletype/archives/2006/06/encyclopedias_s.html#comments">Gelman</a>'s blog)

Comparing arcticles would be a great excuse for <a target="_blank" title="phd" href="http://www.phdcomics.com/comics.php">procrastinating</a>.]]></content:encoded>
<wp:post_id>54</wp:post_id>
<wp:post_date>2006-06-15 13:30:58</wp:post_date>
<wp:post_date_gmt>2006-06-15 12:30:58</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>mathematical-encyclopaedia</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>58</wp:comment_id>
<wp:comment_author><![CDATA[Yaroslav Bulatov]]></wp:comment_author>
<wp:comment_author_email>yaroslavvb@gmail.com</wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>65.197.143.217</wp:comment_author_IP>
<wp:comment_date>2006-09-01 01:28:15</wp:comment_date>
<wp:comment_date_gmt>2006-09-01 00:28:15</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Also PlanetMath
http://planetmath.org/]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>The US FDA is becoming progressively more Bayesian</title>
<link>http://yamlb.wordpress.com/2006/06/19/the-us-fda-is-becoming-progressively-more-bayesian/</link>
<pubDate>Mon, 19 Jun 2006 15:47:06 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/06/19/the-us-fda-is-becoming-progressively-more-bayesian/</guid>
<description></description>
<content:encoded><![CDATA[The US <a class="l" href="http://www.fda.gov/"><strong>Food and Drug Administration</strong></a><font size="-1"> extensively uses statistical science, for instance for testing new drugs....  They mainly uses classical hypothesis testing  and other frequencists procedure, but it seems that the bayesian methodology is becoming more popular. They recently published a <a target="_blank" href="http://www.fda.gov/cdrh/osb/guidance/1601.html">Draft Guidance on Bayesian statistics</a>. FDA is a leading authority as regards applied statitics.</font>

This draft contains a lot of interesting remarks, showing the FDA point of vue:
<blockquote>Bayesian statistics is a statistical theory and approach to data analysis   that provides a coherent method for learning from evidence as it accumulates.</blockquote>
<blockquote>Bayesian methods may be controversial when the prior information is based   mainly on personal opinion (often derived by elicitation methods). The methods   are often not controversial when the prior information is based on empirical   evidence such as prior clinical trials.</blockquote>
<blockquote>The Bayesian approach, when correctly employed, may be less burdensome than a frequentist approach</blockquote>
<blockquote>The Bayesian methodology may reduce the sample size FDA needs to reach a   regulatory decision.</blockquote>
<blockquote>With appropriate planning, the Bayesian approach can also offer the flexibility   of midcourse changes to a trial.</blockquote>
<blockquote>A change in the prior information or the model at a later stage of the trial   may imperil the scientific validity of the trial results.</blockquote>
<blockquote>The use of Bayesian hierarchical models enables us to combine information   from different sources that may be exchangeable on some levels but not on others</blockquote>
<blockquote>Since the software used in Bayesian analysis is relatively new, FDA will   often verify results using alternate software.</blockquote>
<blockquote>the frequentist approach   makes extensive use of the likelihood function, it does not always strictly   adhere to the likelihood principle.</blockquote>
<blockquote>Another way of saying this is that Bayesian inferences are based on the “parameter   space”, while frequentist inferences are   based on the “sample space”</blockquote>
<blockquote>In traditional frequentist clinical trial design, the sample   size is determined in advance. Instead of specifying a particular sample size,   the Bayesian approach (and some modern frequentist methods) may specify a particular criterion to stop the trial.</blockquote>
<blockquote>FDA believes the Bayesian approach is well suited for surveillance purposes.</blockquote>
They also provide some recommandation for designing a good bayesian data analysis procedure.]]></content:encoded>
<wp:post_id>55</wp:post_id>
<wp:post_date>2006-06-19 16:47:06</wp:post_date>
<wp:post_date_gmt>2006-06-19 15:47:06</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>the-us-fda-is-becoming-progressively-more-bayesian</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Finding good priors is a sloved issue!</title>
<link>http://yamlb.wordpress.com/2006/08/07/finding-good-priors-is-a-sloved-issue/</link>
<pubDate>Mon, 07 Aug 2006 15:22:31 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Misc]]></category>

		<category domain="category" nicename="misc"><![CDATA[Misc]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/08/07/finding-good-priors-is-a-sloved-issue/</guid>
<description></description>
<content:encoded><![CDATA[From  <a target="_blank" href="http://yudkowsky.net/bayes/bayes.html">Eliezer S. Yudkowsky</a>.
<blockquote><strong>Q.  What is the Bayesian Conspiracy?</strong>
A.  The Bayesian Conspiracy is a multinational,
interdisciplinary, and shadowy group of scientists
that controls publication, grants, tenure, and the
illicit traffic in grad students.  The best way to be
accepted into the Bayesian Conspiracy is to join the
Campus Crusade for Bayes in high school or college,
and gradually work your way up to the inner circles.
It is rumored that at the upper levels of the Bayesian
Conspiracy exist nine silent figures known only as the
Bayes Council.

<strong>Q.  How can I find the priors for a problem?</strong>
A.  Many commonly used priors are listed in the
Handbook of Chemistry and Physics.

<strong>Q.  Where do priors originally come from?</strong>
A.  Never ask that question.

<strong>Q.  Uh huh.  Then where do scientists get their priors?</strong>
A.  Priors for scientific problems are established by
annual vote of the AAAS.  In recent years the vote has
become fractious and controversial, with widespread
acrimony, factional polarization, and several outright
assassinations.  This may be a front for infighting
within the Bayes Council, or it may be that the
disputants have too much spare time.  No one is really
sure.

<strong>Q.  I see.  And where does everyone else get their priors?</strong>
A.  They download their priors from Kazaa.

<strong>Q.  What if the priors I want aren't available on Kazaa?</strong>
A.  There's a small, cluttered antique shop in a back
alley of San Francisco's Chinatown.  Don't ask about
the bronze rat.

<strong>Q.  Why did the Bayesian reasoner cross the road?</strong>
A.  You need more information to answer this question.

<strong>Q.  Are there any limits to the power of Bayes' Theorem?</strong>
A.  According to legend, one who fully grasped Bayes'
Theorem would gain the ability to create and
physically enter an alternate universe using only
off-the-shelf equipment and a short computer program.
One who fully grasps Bayes' Theorem, yet remains in
our universe to aid others, is known as a Bayesattva.</blockquote>]]></content:encoded>
<wp:post_id>56</wp:post_id>
<wp:post_date>2006-08-07 16:22:31</wp:post_date>
<wp:post_date_gmt>2006-08-07 15:22:31</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>finding-good-priors-is-a-sloved-issue</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Machine Learning List: Volume 18, Number 6</title>
<link>http://yamlb.wordpress.com/2006/09/05/machine-learning-list-volume-18-number-6/</link>
<pubDate>Tue, 05 Sep 2006 16:29:26 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Events]]></category>

		<category domain="category" nicename="events"><![CDATA[Events]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/09/05/machine-learning-list-volume-18-number-6/</guid>
<description></description>
<content:encoded><![CDATA[The Machine Learning List is a moderated mailing list announcing conferences and events.

Contributions should be relevant to the scientific study of machine learning. Please send submissions for distribution to: <a class="moz-txt-link-abbreviated" href="mailto:ml@isle.org">ml@isle.org</a>. For requests to be added, removed, or to change your email address, send email to: <a class="moz-txt-link-abbreviated" href="mailto:ml-request@isle.org">ml-request@isle.org</a>.
<blockquote>***********************************************************
Machine Learning List: Volume 18, Number 6
Friday, September 1, 2006
***********************************************************

Contents
Calls for Papers and Participation
Workshop on Neural-Symbolic Learning and Reasoning
Conference on Semantics and Digital Media
Matching Systems Participation
Artificial Intelligence in Medicine
IJCAI Workshop on Knowledge and Reasoning in Dialogue Systems
Special Issue on CBR in the Health Sciences
11th International Conference on User Modeling
Foundations of Genetic Algorithms
Symposium on Artificial Neural Networks
Journal of Information Technology and Web Engineering
Chapters on Computational Methods of Feature Selection
Interactive Mobile and Computer-Aided Learning
Workshop on Learning for Architectures and Compilation
EVO* 2007
Intelligent User Interface
ECML/PKDD 2008/2009
Career Opportunities
Postdoctoral Fellowships in Machine Learning at UCL
Postdoctoral Position at Carnegie Mellon University
Postdoctoral Position in Computational Sensorimotor Integration
Research Vacancies at University of Cyprus
**************************************************************

The Machine Learning List is moderated. Contributions should be relevant
to the scientific study of machine learning. Please send submissions for
distribution to: <a class="moz-txt-link-abbreviated" href="mailto:ml@isle.org">ml@isle.org</a>. For requests to be added, removed, or to
change your email address, send email to: <a class="moz-txt-link-abbreviated" href="mailto:ml-request@isle.org">ml-request@isle.org</a>. To keep
mailings to a manageable size, please keep submissions brief. For meeting
announcements, we will publicize only the name, location, URL, and key
dates. The ML List moderator reserves the right to omit/edit submissions
to meet these criteria.
**************************************************************

Date: Jul 9, 2006
From: Pascal Hitzler <a class="moz-txt-link-rfc2396E" href="mailto:hitzler@aifb.uni-karlsruhe.de" />
Subject: Workshop on Neural-Symbolic Learning and Reasoning

CALL FOR PAPERS
Third International Workshop on Neural-Symbolic Learning and Reasoning
Hyderabad, India, January 8, 2007
<a class="moz-txt-link-freetext" href="http://www.neural-symbolic.org/NeSy07/">http://www.neural-symbolic.org/NeSy07/</a>

Submission Deadline: September 26,2006
-------------------------------------------------------------------------

Date: Jul 10, 2006
From: <a class="moz-txt-link-abbreviated" href="mailto:publicity@samt2006.org">publicity@samt2006.org</a> <a class="moz-txt-link-rfc2396E" href="mailto:publicity@samt2006.org" />
Subject: Conference on Semantics and Digital MediaCALL FOR POSTERS AND DEMOS
First International Conference on Semantics and Digital
Media Technology (SAMT 2006)
Former European Workshop on the Integration of knowledge,
Semantic and Digital Media Technologies (EWIMT)
Athens, Greece, December 6-8, 2006
<a class="moz-txt-link-freetext" href="http://samt2006.org/">http://samt2006.org/</a>

Paper/Poster/Demo Submission Deadline: September 1, 2006
Camera-Ready Deadline: November 3, 2006
-------------------------------------------------------------------------

Date: Jul 12, 2006
From: pavel <a class="moz-txt-link-rfc2396E" href="mailto:pavel@dit.unitn.it" />
Subject: Matching Systems ParticipationTHE OAEI'06 CAMPAIGN
CALL FOR MATCHING SYSTEMS PARTICIPATION
Athens, GA, November 5, 2006
<a class="moz-txt-link-freetext" href="http://oaei.ontologymatching.org/2006/">http://oaei.ontologymatching.org/2006/</a>
-------------------------------------------------------------------------

Date: Jul 20, 2006
From: Riccardo Bellazzi <a class="moz-txt-link-rfc2396E" href="mailto:riccardo.bellazzi@unipv.it" />
Subject: Artificial Intelligence in Medicine

11th Conference on Artificial Intelligence in Medicine (AIME 07)
Amsterdam, The Netherlands, July 7-11, 2007
Web site: <a class="moz-txt-link-freetext" href="http://www.aimedicine.eu/aime07">http://www.aimedicine.eu/aime07</a>

Abstract Submission Deadline: January 23, 2007
Paper Submission Deadline: January 31, 2007
Camera-ready Deadline: April 17, 2007
-------------------------------------------------------------------------

Date: Jul 24, 2006
From: Ingrid Zukerman <a class="moz-txt-link-rfc2396E" href="mailto:Ingrid.Zukerman@infotech.monash.edu.au" />
Subject: IJCAI Workshop on Knowledge and Reasoning in Dialogue Systems

Call for Papers
20th International Conference on Artificial Intelligence
Hyderabad, India, in January 6-12, 2007
5th IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems
Hyderabad, India, January 8, 2007

Paper Submission Deadline: September 25, 2006
-------------------------------------------------------------------------

Date: Jul 25, 2006
From: Stefania Montani <a class="moz-txt-link-rfc2396E" href="mailto:stefania.montani@unipmn.it" />
Subject: Special Issue on CBR in the Health Sciences

Call for Papers
Applied Intelligence Special Issue on CBR in the Health Sciences
<a class="moz-txt-link-freetext" href="http://faculty.washington.edu/ibichind/ai07/">http://faculty.washington.edu/ibichind/ai07/</a>

Paper Submission Deadline: October 15, 2006
Camera-ready Deadline: February 15, 2007
-------------------------------------------------------------------------

Date: Jul 27, 2006
From: George Paliouras <a class="moz-txt-link-rfc2396E" href="mailto:paliourg@iit.demokritos.gr" />
Subject: 11th International Conference on User Modeling11th International Conference on User Modeling (UM2007)
Corfu, Greece, June 25-29, 2007
<a class="moz-txt-link-freetext" href="http://www.iit.demokritos.gr/um2007">http://www.iit.demokritos.gr/um2007</a>

Paper/Poster Submission Deadline: November 1, 2006
Workshop Proposal Submission Deadline: November 6, 2006
Tutorial Proposal Submission Deadline: November 6, 2006
Doctoral Consortium Paper Submission Deadline: December 28, 2006
-------------------------------------------------------------------------

Date: Aug 1, 2006
From: Marc Toussaint <a class="moz-txt-link-rfc2396E" href="mailto:mtoussai@inf.ed.ac.uk" />
Subject: Foundations of Genetic Algorithms

Call for Papers
Foundations of Genetic Algorithms (FOGA IX)
Mexico City, Mexico, January 7-11, 2007
<a class="moz-txt-link-freetext" href="http://www.sigevo.org/foga-2007/index.html">http://www.sigevo.org/foga-2007/index.html</a>

Extended Abstracts Deadline: September 20, 2006
Registration Deadline: September 20, 2006
Full Papers Submission Deadline: December 1, 2006
-------------------------------------------------------------------------

Date: Aug 1, 2006
From: <a class="moz-txt-link-abbreviated" href="mailto:esann@dice.ucl.ac.be">esann@dice.ucl.ac.be</a>
Subject: Symposium on Artificial Neural Networks

Call for special sessions
15th European Symposium on Artificial Neural Networks (ESANN 07)
Advances in Computational Intelligence and Learning
Bruges, Belgium, April 25-27, 2007
<a class="moz-txt-link-freetext" href="http://www.dice.ucl.ac.be/esann/">http://www.dice.ucl.ac.be/esann/</a>

Proposal Deadline: August 31, 2006
-------------------------------------------------------------------------

Date: Aug 10, 2006
From: Michael Fitzgerald Nowlan <a class="moz-txt-link-rfc2396E" href="mailto:mfn3@georgetown.edu" />
Subject: Journal of Information Technology and Web Engineering

International Journal of Information Technology and Web Engineering

Call for papers to special issues:
Optimizing Web-based Query and Information Retrieval
December 31, 2006
Web-based e-Learning Systems
March 31, 2007
Current Trends in Web Engineering Design and Applications
July 31, 2007
Usability Engineering and Human-Computer Interactions for
Web-based Systems
October 31, 2007
Grid Computing and Web-based Systems
January 31, 2008
<a class="moz-txt-link-freetext" href="http://www.idea-group.com/ijitwe">http://www.idea-group.com/ijitwe</a>
-------------------------------------------------------------------------

Date: Aug 14, 2006 1:20 AM
From: Hiroshi Motoda <a class="moz-txt-link-rfc2396E" href="mailto:motoda@ar.sanken.osaka-u.ac.jp" />
Subject: Chapters on Computational Methods of Feature Selection

Call for Book Chapters on Computational Methods of Feature Selection

Submission Deadline: December 31, 2006
Camera-ready Deadline: April 15, 2007

The book will be published by Chapman and Hall/CRC Press,
and edited by Huan Liu and Hiroshi Motoda.
<a class="moz-txt-link-freetext" href="http://www.public.asu.edu/%7Ehuanliu/Book2007/instructions.html">http://www.public.asu.edu/~huanliu/Book2007/instructions.html</a>.
-------------------------------------------------------------------------

Date: Aug 15, 2006
From: <a class="moz-txt-link-abbreviated" href="mailto:IMCL2007@psut.edu.jo">IMCL2007@psut.edu.jo</a>
Subject: Interactive Mobile and Computer-Aided Learning

Call for Papers
2nd International Conference on Interactive Mobile
and Computer-Aided Learning (IMCL 2007)
Amman, Jordan, April 18-20, 2007
<a class="moz-txt-link-abbreviated" href="http://www.imcl-conference.org/">www.imcl-conference.org</a>

Deadline for Abstracts: November 15, 2006
Deadline for Workshops/Tutorials/Thematic sessions: December 5, 2006
Deadline for Author Registration: January 10, 2007
Camera-Ready Deadline: March 5, 2007
-------------------------------------------------------------------------

Date: Aug 20, 2006
From: John Cavazos <a class="moz-txt-link-rfc2396E" href="mailto:jcavazos@inf.ed.ac.uk" />
Subject: Workshop on Learning for Architectures and Compilation

Call for Papers
First Workshop on Statistical and Machine Learning Approaches
Applied to Architectures and Compilation (SMART '07)
Ghent, Belgium, January 28, 2007
<a class="moz-txt-link-freetext" href="http://www.hipeac.net/smart-workshop.html">http://www.hipeac.net/smart-workshop.html</a>
Co-located with International Conference on High Performance
Embedded Architectures &amp; Compilers (HiPEAC 2007)
Ghent, Belgium, January 28-30, 2007
<a class="moz-txt-link-freetext" href="http://www.hipeac.net/conference/">http://www.hipeac.net/conference/</a>

Submission Deadline: October 20, 2006
-------------------------------------------------------------------------

Date: Aug 28, 2006
From: Leonardo Vanneschi <a class="moz-txt-link-rfc2396E" href="mailto:vanneschi@disco.unimib.it" />
Subject: EVO* 2007

Call for Papers
EVO* 2007 including EuroGP, EvoCOP,
EvoBIO and Evo Workshops
Valencia, Spain, 11-13 April, 2007,
<a class="moz-txt-link-freetext" href="http://www.evostar.org/">http://www.evostar.org</a>

Submission Deadline: November 1,2006
Camera-ready Deadline: January 8, 2007
-------------------------------------------------------------------------

Date: Aug 28, 2006
From: Fang Chen <a class="moz-txt-link-rfc2396E" href="mailto:Fang.Chen@nicta.com.au" />
Subject: International Conference on Intelligent User Interfaces

CALL FOR PARTICIPATION
International Conference on Intelligent User Interfaces (IUI 2007)
Hawaii, USA, January 28-31, 2007
<a class="moz-txt-link-freetext" href="http://iuiconf.org/">http://iuiconf.org</a>

Short and Long Paper Deadline: September 18, 2006, 11:59 pm PDT
Tutorial Proposal Deadline: August 4, 2006, 11:59pm PDT
Workshop Proposal Deadline: August 14, 2006, 11:59pm PDT
-------------------------------------------------------------------------

Date: Aug 9, 2006 6:23 PM
From: <a class="moz-txt-link-abbreviated" href="mailto:juffi@ke.informatik.tu-darmstadt.de">juffi@ke.informatik.tu-darmstadt.de</a>
Subject: ECML/PKDD 2008/2009

CALL FOR PROPOSALS
European Conference on Machine Learning
and European Conference on Principles and Practice of
Knowledge Discovery in Databases (ECML/PKDD 2008)
CALL FOR EXPRESSIONS OF INTEREST
European Conference on Machine Learning
and European Conference on Principles and Practice of
Knowledge Discovery in Databases (ECML/PKDD 2009)

The purpose of this call is to invite groups interested in
hosting the next events in the ECML/PKDD conference series.
The ECML/PKDD conferences have been co-located since 2001,
and proposals that continue this tradition will be strongly
preferred. ECML/PKDD 2006 (<a class="moz-txt-link-freetext" href="http://www.ecmlpkdd2006.org/">http://www.ecmlpkdd2006.org/</a>)
will take place in Berlin, September 18-22,2006, ECML/PKDD 2007
(<a class="moz-txt-link-freetext" href="http://www.ecmlpkdd2007.org/">http://www.ecmlpkdd2007.org/</a>) will take place in Warsaw,
Poland, September 17-21,2007.

All proposals will be presented and discussed at the business
meeting of ECML/PKDD 2006 in Berlin, which takes place on
Thursday, September 21, 2006. The community will then decide
upon the location and the chairs of the meeting in 2008, and
will discuss and provide feedback on expressions of interest
for organizing the meeting in 2009. Prior to the presentation
at the community meeting in Berlin, the ECML/PKDD Steering
Committee and the conference chairs of ECML/PKDD 2006 and
2007 will review the proposals and may provide informal feedback
to the organizers.

FORMAT OF PROPOSALS

Expressions of interest are informal and should briefly
describe the planned event, the location, and the organizing
committee.

Full proposals should address the following issues:

1. Proposed Dates
The conference should be scheduled for five days
(two days reserved mostly for workshops and tutorials;
three days for paper sessions and invited talks), preferably
in fall 2008.
2. Local Parameters
- Accessibility - Is it easy and inexpensive for people
(especially graduate students) to travel to the conference
site?  (Compute mean airfares from Europe, but also North
America, and Asia. Include ground transportation from relevant
airports to the site.)
- Meeting Rooms, AV Equipment, etc. - What are the physical
facilities like?  Consider rooms for plenary sessions, parallel
sessions, workshops, tutorials, and poster sessions.  What are
the charges, if any, for using them?
- Meals and Lodging - Is there low-cost, quality housing available
for attendees (especially graduate students)?  How far from the
meeting rooms?  Where will attendees eat?  Please estimate costs
for meals and lodging.
- Demo facilities - Will there be computing equipment and space
available to support demos?
- Other features - You may mention any other aspects of the site or
the region that are relevant.
3. Local ML/KDD Community. Is there a local ML or KDD group/community
that will be involved in the organization and funding?
4. Opportunities to co-locate with other conferences.
5. Organizational and Institutional Support. Is there a conference
office that can help with local arrangements?

Proposals should be addressed to the program chairs of ECML/PKDD
2006 (E-mail: <a class="moz-txt-link-abbreviated" href="mailto:pcchairs@ecmlpkdd2006.org">pcchairs@ecmlpkdd2006.org</a>)

Submission Deadline: September 5, 2006
-------------------------------------------------------------------------

Date: Aug 8, 2006
From: Yee Whye Teh <a class="moz-txt-link-rfc2396E" href="mailto:yeewhye@gmail.com" />
Subject: Postdoctoral Fellowships in Machine Learning at UCL

The Gatsby Computational Neuroscience Unit invites applications
for postdoctoral training fellowships in machine learning and
related areas. The Unit is especially keen to recruit researchers
with expertise in graphical models, Bayesian statistics, nonparametric
methods, kernel methods, semi-supervised learning, reinforcement
learning, game theory or machine learning applied to neural data,
natural language processing, machine vision or bioinformatics.

The Unit is a world-class centre for theoretical neuroscience and
machine learning. Yee Whye Teh will join the faculty in January 2007.
For further details of our research please see:
<a class="moz-txt-link-freetext" href="http://www.gatsby.ucl.ac.uk/research.html">http://www.gatsby.ucl.ac.uk/research.html</a>

The Unit provides a unique environment in which a critical mass of
theoreticians interact closely with each other and with other
world-class research groups in related departments at University
College London. The Unit's visitor and seminar programmes enable
staff and students to engage with leading researchers from across
the world.

Candidates must have a strong analytical background and demonstrable
interest and expertise in statistical machine learning.

Stipend according to experience and achievement. Funding for the
fellowships is available for up to two years.

Applicants should send in pdf, plain text or Word format a CV, a
statement of research interests, and the names and full contact
details (including e-mail addresses) for three academic referees to:
<a class="moz-txt-link-abbreviated" href="mailto:asstadmin@gatsby.ucl.ac.uk">asstadmin@gatsby.ucl.ac.uk</a>

Applicants are directed to further particulars about the positions
available from: <a class="moz-txt-link-freetext" href="http://www.gatsby.ucl.ac.uk/vacancies/">http://www.gatsby.ucl.ac.uk/vacancies/</a>; academic
enquiries can be directed to <a class="moz-txt-link-abbreviated" href="mailto:ywteh@gatsby.ucl.ac.uk">ywteh@gatsby.ucl.ac.uk</a>
<a class="moz-txt-link-freetext" href="http://www.gatsby.ucl.ac.uk/">http://www.gatsby.ucl.ac.uk/</a>

Closing date for applications: September 14, 2006
-------------------------------------------------------------------------

Date: Aug 10, 2006
From: Bruce McLaren <a class="moz-txt-link-rfc2396E" href="mailto:bmclaren@cs.cmu.edu" />
Subject: Postdoctoral Position at Carnegie Mellon University

The School of Computer Science at Carnegie Mellon University
has an opening for a post-doc in a new project focused on
the development of usable technologies that empower lay users
to effectively control privacy and security policies in
pervasive computing environments. This is an interdisciplinary
project that involves interacting with a number of faculty and
students and combines the development and evaluation of learning
and dialog technologies with the use of human-centered design
methodologies.

Prospective candidates are expected to have done graduate
work/have some familiarity with one or more of the following areas:
Learning and dialog technologies (e.g. conversational case-based
reasoning)
Mobile and Pervasive Computing
Web Security and Privacy
Human Computer Interaction

Candidates are expected to be capable of working under limited
supervision and should have strong communication skills. They
are also expected to have strong Java programming skills and
have experience collaborating with others in the context of
large-scale software development projects. Actual work is expected
to involve a mix of conceptual design, software development and
field evaluation with human subjects. The successful candidate is
expected to publish papers in top level conferences and journals
along with other faculty and students.

Initial appointments are for one year with option of renewal. The
start date is negotiable and can be anytime during the second half
of 2006, with a preference for an early start.

Carnegie Mellon offers competitive salaries and benefits.

Interested candidates should forward their resume along with the
names of three references by email, fax, or snail mail to:

Prof. Norman Sadeh
ISR, School of Computer Science
5000 Forbes Avenue
Pittsburgh, PA 15213-3891 USA
Fax: +1-412-291-1110
Email: <a class="moz-txt-link-abbreviated" href="mailto:sadeh@cs.cmu.edu">sadeh@cs.cmu.edu</a>
<a class="moz-txt-link-freetext" href="http://www.cs.cmu.edu/%7Esadeh/">http://www.cs.cmu.edu/~sadeh/</a>
-------------------------------------------------------------------------

Date: Aug 10, 2006
From: Konrad Koerding <a class="moz-txt-link-rfc2396E" href="mailto:kording@mit.edu" />
Subject: Postdoctoral Position in Computational Sensorimotor Integration

An enthusiastic and well-qualified post-doctoral researcher is
required to work in the wide area of computational sensorimotor
integration in the laboratory of Konrad Koerding at the Rehabilitation
Institute of Chicago, Northwestern University, Chicago.  The research
in the laboratory focuses on the question how people combine
information from several sensors to allow them to move efficiently.
Towards this aim new Bayesian models are developed and tested using
human psychophysics. The successful applicant will be expected to work
on computational models and experimental studies in humans.

Applicants should have a PhD and skills relevant to machine learning,
Bayesian statistics or computational neuroscience and an interest
in the way the nervous system solves computational problems; however,
applicants with a strong background in psychophysics who wish to learn
computational approaches will also be considered. Experience with
Matlab would be very advantageous.

The Rehabilitation Institute of Chicago (RIC) boasts a large
concentration of researchers that address various questions
about movement in healthy and patient populations. The
department of physiology hosts many scientists probing the
neural basis of movement.

Informal inquiries can be addressed by email to Professor Konrad
Koerding (<a class="moz-txt-link-abbreviated" href="mailto:konrad@koerding.com">konrad@koerding.com</a>). For more information about the
environment see the Web Pages of the Rehabilitation Institute of
Chicago (<a class="moz-txt-link-abbreviated" href="http://www.ric.org/">www.ric.org</a>), the Northwestern Department of Physiology
(<a class="moz-txt-link-abbreviated" href="http://www.physio.northwestern.edu/">www.physio.northwestern.edu</a>) and the Northwestern Department of
Physical Medicine and Rehabilitation (<a class="moz-txt-link-abbreviated" href="http://www.northwestern.edu/pmr/">www.northwestern.edu/pmr/</a>).

The Rehabilitation Institute of Chicago is an Affirmative
Action - Equal Opportunity Employer. Applicants are considered for
all positions, and employees are treated during employment without
regard to race, color, religion, gender, national origin, age,
marital or veteran status, the presence of a non-job-related medical
condition, disability, sexual orientation, or any other legally
protected status to the extent required by law. The Rehabilitation
Institute of Chicago embraces diversity in its work force. The
position is available from October 2006 for one year in the
first instance.

Applicants should supply:
- one-page statement of research interests
- Copy of CV (two if sent by post)
- Names and contact details of three references
--------------------------------------------------------------------------

Date: Aug 24, 2006
From: George Papadopoulos <a class="moz-txt-link-rfc2396E" href="mailto:george@cs.ucy.ac.cy" />
Subject: Research Vacancies at University of Cyprus

The Department of Computer Science intends to announce shortly
a number of vacancies at the level of Research Associate (RA).
These RAs will join the research group of the Software Engineering
and Internet Technologies (SEIT) Lab (<a class="moz-txt-link-freetext" href="http://www.cs.ucy.ac.cy/seit">http://www.cs.ucy.ac.cy/seit</a>)
and will be assigned to the EU funded project MUSIC (Self-Adapting
Applications for Mobile Users in Ubiquitous Computing Environments).
MUSIC is an Integrated Project, funded by FP6 IST, with a duration
of 42 months and a budget of 14.5 MEUROs. It's purpose is to develop
an open-source framework facilitating the development of adaptive,
reconfigurable software. The project is expected to commence in
October 2006.

Each position is for one year, renewable  annually up to the end
of the project. The salary depends on qualifications and previous
experience and ranges between 11,000 - 20,000 CY pounds per annum
(1 CY = 1.75 EUR).

These posts are suitable for both holders of a doctorate degree
who wish to pursue a post doc and for graduates who wish to pursue
a Ph.D. For the former case, the prospective candidate should have
a Ph.D. in Computer Science and a number of good quality publications.
For the latter case, the prospective candidate should have a B.Sc.
and preferably also a  M.Sc. in Computer Science and evidence of
commitment to  research. In both cases, the candidate should be
sufficiently familiar in one or  more of the following areas:
Software Engineering, Parallel and Distributed Systems, Mobile
Systems, Component-Based Software Development and Middleware
Platforms, Internet and Service-Oriented Computing.

Potential candidates are encouraged to send a full CV to
Professor George A. Papadopoulos (<a class="moz-txt-link-abbreviated" href="mailto:george@cs.ucy.ac.cy">george@cs.ucy.ac.cy</a>) at
their earliest convenience.
-------------------------------------
End of ML-LIST Digest Vol 18, No. 6</blockquote>]]></content:encoded>
<wp:post_id>57</wp:post_id>
<wp:post_date>2006-09-05 17:29:26</wp:post_date>
<wp:post_date_gmt>2006-09-05 16:29:26</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>machine-learning-list-volume-18-number-6</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Disapointed by your Netflix results ?</title>
<link>http://yamlb.wordpress.com/2006/11/24/disapointed-by-your-netflix-results/</link>
<pubDate>Fri, 24 Nov 2006 09:51:12 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Modeling]]></category>

		<category domain="category" nicename="modeling"><![CDATA[Modeling]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/11/24/disapointed-by-your-netflix-results/</guid>
<description></description>
<content:encoded><![CDATA[<img align="middle" alt="occam_razor" src="http://emotion.inrialpes.fr/~dangauthier/images/occam_razor_small.jpg" />]]></content:encoded>
<wp:post_id>58</wp:post_id>
<wp:post_date>2006-11-24 10:51:12</wp:post_date>
<wp:post_date_gmt>2006-11-24 09:51:12</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>disapointed-by-your-netflix-results</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>59</wp:comment_id>
<wp:comment_author><![CDATA[rb]]></wp:comment_author>
<wp:comment_author_email>fake@fake.com</wp:comment_author_email>
<wp:comment_author_url></wp:comment_author_url>
<wp:comment_author_IP>88.154.92.22</wp:comment_author_IP>
<wp:comment_date>2006-11-24 11:00:56</wp:comment_date>
<wp:comment_date_gmt>2006-11-24 10:00:56</wp:comment_date_gmt>
<wp:comment_content><![CDATA[:)]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>60</wp:comment_id>
<wp:comment_author><![CDATA[Igor]]></wp:comment_author>
<wp:comment_author_email>Igorcarron@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://nuit-blanche.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>82.123.49.6</wp:comment_author_IP>
<wp:comment_date>2006-11-24 20:15:56</wp:comment_date>
<wp:comment_date_gmt>2006-11-24 19:15:56</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Peut-etre que les classements humains ne sont pas Bayesiens ?

Igor.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>61</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>82.228.182.110</wp:comment_author_IP>
<wp:comment_date>2006-11-25 15:26:49</wp:comment_date>
<wp:comment_date_gmt>2006-11-25 14:26:49</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Maybe, but whatever the method you use for the Netflix contest, it will have to generalize to unseen cases. Thus your method could be subject to overfitting, and penalizing too complicated models (Occam Razor) could improve prediction accuracy.
On the other hand a lot of current research try to show that bayesian inference is a good model for the human decision process (ex:<a title="BACS" rel="nofollow" href="http://www.bacs.ethz.ch">BACS</a>, <a target="_blank" href="http://www.koerding.com/pubs/koerdingNature2004.pdf">Nature letter</a>).]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>62</wp:comment_id>
<wp:comment_author><![CDATA[Igor]]></wp:comment_author>
<wp:comment_author_email>Igorcarron@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://nuit-blanche.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>82.123.191.195</wp:comment_author_IP>
<wp:comment_date>2006-11-25 18:00:44</wp:comment_date>
<wp:comment_date_gmt>2006-11-25 17:00:44</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Pierre,

I perfectly understand your point, however one of the item that bothers me is that one has to explain to the average person why it is to their advantage to choose the other door in the three door problem ? if you have to give an explanation that is, the average person, counterintuitive, then, maybe, the human decision process does not map directly to a bayesian one.

Enfin c'est ce que j'en pense.

Igor.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>63</wp:comment_id>
<wp:comment_author><![CDATA[Pierre]]></wp:comment_author>
<wp:comment_author_email>pierre.dangauthier@laposte.net</wp:comment_author_email>
<wp:comment_author_url>http://emotion.inrialpes.fr/~dangauthier/</wp:comment_author_url>
<wp:comment_author_IP>82.228.182.110</wp:comment_author_IP>
<wp:comment_date>2006-11-25 18:50:51</wp:comment_date>
<wp:comment_date_gmt>2006-11-25 17:50:51</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Sure, humans are sometimes taking wrong decisions! The kind of research I mentioned are more focussed on low-level behavior, like low-level vision, motor control... Regarding high-level problems like logical puzzles, mathematics etc, it's much more difficult to model the decision making process ;-).
Coming back to the Netflix pb, simple model could however be stated , for instance "I usually love action movies". Such kind patterns can be found by a statistical analysis of the data set, and, why not, a bayesian anlysis.
Movie taste can be predicted in this case, as it's shown by contest leaders results.
ps: merci pour les commentaires et pardon si je n ai pas bien compris vos propos]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>64</wp:comment_id>
<wp:comment_author><![CDATA[Shane]]></wp:comment_author>
<wp:comment_author_email>shane@sbutler.com</wp:comment_author_email>
<wp:comment_author_url>http://sbutler.com/blog</wp:comment_author_url>
<wp:comment_author_IP>203.36.157.8</wp:comment_author_IP>
<wp:comment_date>2007-01-03 05:37:33</wp:comment_date>
<wp:comment_date_gmt>2007-01-03 04:37:33</wp:comment_date_gmt>
<wp:comment_content><![CDATA[Haha nice :)]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>65</wp:comment_id>
<wp:comment_author><![CDATA[Igor Carron]]></wp:comment_author>
<wp:comment_author_email>igorcarron@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://nuit-blanche.blogspot.com</wp:comment_author_url>
<wp:comment_author_IP>83.202.116.224</wp:comment_author_IP>
<wp:comment_date>2007-02-15 20:27:19</wp:comment_date>
<wp:comment_date_gmt>2007-02-15 19:27:19</wp:comment_date_gmt>
<wp:comment_content><![CDATA[You want an inference problem that is important, try this one:
- There is this boat that we think is following currents (no sail, no engine). You have a model for these currents.

- Several satellites, planes have flown over the area and you have gotten hits, that they have seen something but we do not know if this is what we are looking for. It could be another boat going from one place to the other,....
In effect, you have two areas that have been covered and for which you have several hits.

Which of the hits in the first image is also the same hit on the second image. The two images overlap.


http://nuit-blanche.blogspot.com/2007/02/finding-jim-gray-approximate-data.html

Igor.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
<wp:comment>
<wp:comment_id>66</wp:comment_id>
<wp:comment_author><![CDATA[Igor Carron]]></wp:comment_author>
<wp:comment_author_email>igorcarron@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://nuit-blanche.blogspot.com/2007/02/finding-jim-gray-first-tenacious.html</wp:comment_author_url>
<wp:comment_author_IP>82.123.162.117</wp:comment_author_IP>
<wp:comment_date>2007-02-17 13:37:58</wp:comment_date>
<wp:comment_date_gmt>2007-02-17 12:37:58</wp:comment_date_gmt>
<wp:comment_content><![CDATA[I posted it here:
http://nuit-blanche.blogspot.com/2007/02/finding-jim-gray-first-tenacious.html

Igor.]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>A simple MCMC code in python</title>
<link>http://yamlb.wordpress.com/2006/11/25/a-simple-mcmc-code-in-python/</link>
<pubDate>Sat, 25 Nov 2006 17:33:04 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Inference]]></category>

		<category domain="category" nicename="inference"><![CDATA[Inference]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/11/25/a-simple-mcmc-code-in-python/</guid>
<description></description>
<content:encoded><![CDATA[<strong>What</strong> :Implementation of a simple MCMC <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs</a> sampler.
<strong>Goal</strong> : find the 2 means of mixture of 2 gaussians (with known variances and mixing proportion).
<strong>How</strong> : Compute the bayesian posterior on the means, with a flat prior.
<strong>Gibbs sampling</strong> : introduce a hidden association variable <code>A[i]</code> for each data <code>i</code>. Sample iteratively on <code>A</code>, <code>mean0</code> and <code>mean1</code> during <code>k iterations</code>. After a 'burn in' period, <code> mean0 </code> values are distributed according to the posterior on <code>mean0</code>.
<strong>Results</strong> :
<img width="460" height="346" alt="MCMC posteriors" src="http://emotion.inrialpes.fr/~dangauthier/images/mcmc.png" />

<strong>Details</strong>
We create an artificial data set of <code>N=100</code> 1D points following the mixture
<code>Xi ~ G(x;m0real=10, s0=2) + G(x;m1real=19, s0=5)</code>
We introduce the <code>N</code> hidden variables <code>A[i]</code>.
We choose a flat prior:
<code>mean0 ~ G(x;25, 1000)
mean1 ~ G(x;25, 1000)</code>
During each iteration k, we sample
<code>A_k ~ p(A|mean0_k-1 mean1_k-1) = Bernoulli(OOO)</code> (for each<code> i</code>)
<code>mean0_k ~ p(mean0 | A_k mean1_k-1) = G(x; OOO  , OOO)</code>
<code>mean1_k ~ p(mean1 | A_k mean0_k-1) = G(x; OOO ,  OOO)</code>
Where the <code>OOO</code> parameters are the good one (deduced from the likelihood, I don't bother type the maths here)

Heart of the algorithm:
[python]
for iter in range(1,k+1):
print 'Iteration : ',iter
# Conditional sampling of A
#  A[i]==0 if data[i] is associated to 1st gaussian
#  A[i]==1 if data[i] is associated to 2nd gaussian
A = ones(N)
for i in range(N):
g0 = norm.pdf(data[i], m0list[iter-1],s0)
g1 = norm.pdf(data[i], m1list[iter-1],s1)
p = g1/(g0+g1)
if g0+g1 == 0: # avoiding 'nan'
p = 0.5
A[i] = bernoulli.rvs(p)[0]
# Conditional sampling of m0
tau = lamba_prior/(sig_prior)**2 + sum(data[A==0])/s0**2
pi  = 1/(sig_prior)**2 + len(data[A==0])/s0**2
m0 = norm.rvs(tau/pi, sqrt(1/pi))[0]
# Conditional sampling of m1
tau = lamba_prior/(sig_prior)**2 + sum(data[A==1])/s1**2
pi  = 1/(sig_prior)**2 + len(data[A==1])/s1**2
m1 = norm.rvs(tau/pi, sqrt(1/pi))[0]
[/python]

All the code (with data generation, MCMC, plots and displaying statitics)
[python]
#~ MCMC inference on a simple
#~    mixture of two 1D spherical gaussian
#~ Pierre Dangauthier, 2006
#~ License : GPL

from scipy.stats.distributions import  *
from numpy import *
import pylab as pl

#~ Utils
def MAP(list, domain):
'''Arg Max of the histogram of the list, discretized by domain'''
h = histogram(list, domain)
return (h[1])[argmax(h[0])]

def print_stats(list):
'''Print mean, map, median and variance of a list of values'''
m = median(list)
v = var(list)
s = sqrt(v)
print 'Mean     ',mean(list)
print 'Map      ',MAP(list, arange(m-10*s,m+10*s,20*s/1000))
print 'Median   ',median(list)
print 'Variance ',v

#~ Generating artificial data
n = 100  # half of the data
N = 2*n
m0real = 10.0 # Fisrt gaussian
s0 = 2.0
m1real = 19. # Second gaussian
s1 = 5.0
data = concatenate((norm.rvs(m0real, s0, size=N/2),norm.rvs(m1real, s1, size=N/2)))

#~ Prior (really flat)
lamba_prior = 25.
sig_prior = 1000.

#~ MCMC
k = 100 # nb of mcmc iterations
m0list = [norm.rvs(lamba_prior, sig_prior)[0]]
m1list = [norm.rvs(lamba_prior, sig_prior)[0]]

for iter in range(1,k+1):
print 'Iteration : ',iter
# Conditional sampling of A
#  A[i]==0 if data[i] is associated to 1st gaussian
#  A[i]==1 if data[i] is associated to 2nd gaussian
A = ones(N)
for i in range(N):
g0 = norm.pdf(data[i], m0list[iter-1],s0)
g1 = norm.pdf(data[i], m1list[iter-1],s1)
p = g1/(g0+g1)
if g0+g1 == 0: # avoiding 'nan'
p = 0.5
A[i] = bernoulli.rvs(p)[0]
# Conditional sampling of m0
tau = lamba_prior/(sig_prior)**2 + sum(data[A==0])/s0**2
pi  = 1/(sig_prior)**2 + len(data[A==0])/s0**2
m0 = norm.rvs(tau/pi, sqrt(1/pi))[0]
# Conditional sampling of m1
tau = lamba_prior/(sig_prior)**2 + sum(data[A==1])/s1**2
pi  = 1/(sig_prior)**2 + len(data[A==1])/s1**2
m1 = norm.rvs(tau/pi, sqrt(1/pi))[0]

m0list.append(m0)
m1list.append(m1)

# Printing stats on posteriors
print_stats(m0list[-k/2:])
print_stats(m1list[-k/2:])

#~ # Plotting posteriors
ra = [0,30]
x = arange(ra[0],ra[1],1)

pl.subplot(321)
pl.title('Data = Mixture');
pl.hist(data,arange(ra[0],ra[1],1))

pl.subplot(322)
pl.title('Prior on MU = Gauss(%.2f,%.2f)'%(lamba_prior ,sig_prior ))
pl.plot(x,norm.pdf(x,lamba_prior ,sig_prior ))

pl.subplot(323)
pl.title('MCMC evolution for m0')
pl.plot(m0list)
pl.ylim(median(m0list)-4 ,median(m0list)+4)

pl.subplot(324)
pl.title('MCMC posterior on m0')
pl.xlim(ra[0],ra[1]);
pl.hist(m0list[-k/2:],arange(ra[0],ra[1],0.1),normed=1)

pl.subplot(325)
pl.title('MCMC evolution for m1')
pl.plot(m1list)
pl.ylim(median(m1list)-4 ,median(m1list)+4)

pl.subplot(326)
pl.title('MCMC posterior on m1')
pl.xlim(0,40);
pl.hist(m1list[-k/2:],arange(ra[0],ra[1],0.1),normed=1)

pl.show()
[/python] ]]></content:encoded>
<wp:post_id>59</wp:post_id>
<wp:post_date>2006-11-25 18:33:04</wp:post_date>
<wp:post_date_gmt>2006-11-25 17:33:04</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>a-simple-mcmc-code-in-python</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Probability in a philosophical nutshell</title>
<link>http://yamlb.wordpress.com/2006/12/06/probability-in-a-philosophical-nutshell/</link>
<pubDate>Wed, 06 Dec 2006 00:31:11 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[philosophy]]></category>

		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2006/12/06/probability-in-a-philosophical-nutshell/</guid>
<description></description>
<content:encoded><![CDATA[<blockquote>
<ul>
	<li><a title="Nihilism" href="http://en.wikipedia.org/wiki/Nihilism">Nihilism</a> holds that there is no inherent meaning, value, truth or purpose to the world.</li>
	<li><a title="Agnosticism" href="http://en.wikipedia.org/wiki/Agnosticism">Agnosticism</a> holds that the truth is either not known or inherently unknowable.</li>
	<li><a title="Uncertainty" href="http://en.wikipedia.org/wiki/Uncertainty">Uncertainty</a> is the state of being in doubt.</li>
	<li><strong><a title="Probability" href="http://en.wikipedia.org/wiki/Probability">Probability</a> is a function used to assign odds to that which is unknown by way of what is known.</strong></li>
	<li><a title="Approximation" href="http://en.wikipedia.org/wiki/Approximation">Estimation</a> Deals with inexact representation.</li>
	<li><a title="Belief" href="http://en.wikipedia.org/wiki/Belief">Belief</a> is the state of holding what is not certain to be true.</li>
	<li><a title="Epistemology" href="http://en.wikipedia.org/wiki/Epistemology#Justified_true_belief">Justified true belief</a> is what is both believed and true.</li>
	<li><a title="Certainty" href="http://en.wikipedia.org/wiki/Certainty">Certainty</a> is the state of being without doubt.</li>
	<li><a title="Determinism" href="http://en.wikipedia.org/wiki/Determinism">Determinism</a> holds that all occurances are causally determined by unbroken chains of previous occurances.</li>
</ul>
</blockquote>
From <a target="_blank" title="wikipedia" href="http://en.wikipedia.org/wiki/Template_talk:Certainty">this wikipedia discussion page</a>.]]></content:encoded>
<wp:post_id>60</wp:post_id>
<wp:post_date>2006-12-06 01:31:11</wp:post_date>
<wp:post_date_gmt>2006-12-06 00:31:11</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>probability-in-a-philosophical-nutshell</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Bayesian century</title>
<link>http://yamlb.wordpress.com/2007/01/16/bayesian-century/</link>
<pubDate>Tue, 16 Jan 2007 14:19:51 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[philosophy]]></category>

		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2007/01/16/bayesian-century/</guid>
<description></description>
<content:encoded><![CDATA[<blockquote>Bayesian Rationality: Bayesian rationality is a probabilistic approach to reasoning. Bayesian rationalists describe probability as the degree to which a person should believe a proposition. They also apply Bayes' theorem when inferring or updating their degree of belief when given new information. Some scientists and epistemologists hope to replace the Popperian view of proof with a Bayesian view.</blockquote>
Quoted from <a href="http://sentientdevelopments.blogspot.com/2007/01/must-know-terms-for-21st-century_11.html">this blog.</a>

Interesting in the sense that upgrading the Popperian true/false criteria to a continuous space of degree of beliefs require a shift to a subjective point of vue (the relative weight of prior vs likelihood is a part of the subjective prior).]]></content:encoded>
<wp:post_id>61</wp:post_id>
<wp:post_date>2007-01-16 15:19:51</wp:post_date>
<wp:post_date_gmt>2007-01-16 14:19:51</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>bayesian-century</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Some quotes about entropy</title>
<link>http://yamlb.wordpress.com/2007/02/23/some-quotes-about-entropy/</link>
<pubDate>Fri, 23 Feb 2007 16:08:21 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[philosophy]]></category>

		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2007/02/23/some-quotes-about-entropy/</guid>
<description></description>
<content:encoded><![CDATA[<img width="203" height="292" align="left" alt="boltzmann" src="http://emotion.inrialpes.fr/people/dangauthier/images/boltz.jpg" />"Nothing in life is certain except <strong>death</strong>, <strong>taxes </strong>and the <strong>second law</strong> of thermodynamics. All three are processes in which useful or accessible forms of some quantity, such as energy or money, are transformed into useless, inaccessible forms of the same quantity. That is not to say that these three processes don't have fringe benefits: taxes pay for roads and schools; the second law of thermodynamics drives cars, computers and metabolism; and death, at the very least, opens up tenured faculty positions"
<span style="font-style:italic;">Seth Lloyd, writing in Nature 430, 971 (26 August 2004). </span>

"You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, <strong>nobody knows what entropy really is</strong>, so in a debate you will always have the advantage."<br /> <span style="font-style:italic;">John von Neumann, in a response to Claude Shannon </span>

"Every mathematician knows it is impossible to understand an elementary course in thermodynamics."
<span style="font-style:italic;">V Arnold</span>

"Thermodynamical entropy is no more objective than infomation entropy. Both are as subjective as my opinion on that issue."
<span style="font-style:italic;">Pierre Dangauthier </span>

"Glib, unqualified statements to the effect that "entropy measures randomness" are in my opinion totally meaningless, and present a serious barrier to any real understanding of these problems."
<span style="font-style:italic;">Edwin T Jaynes </span>

"Entropy is an anthropomorphic concept."
<span style="font-style:italic;">Eugene Wigner </span>

It follows from this that the idea of dissipation of energy depends on the extent of our knowledge. Available energy is energy which we can direct into any desired channel. Dissipated energy is energy which we cannot lay hold of and direct at pleasure, such as the energy of the confused agitation of molecules which we call heat. Now, confusion, like the correlative term order, is not a property of material things in themselves, but only in relation to the mind which perceives them. A memorandum-book does not, provided it is neatly written, appear confused to an illiterate person, or to the owner who understands it thoroughly, but to any other person able to read it appears to be inextricably confused. Similarly the notion of dissipated energy could not occur to a being who could not turn any of the energies of nature to his own account, or to one who could trace the motion of every molecule and seize it at the right moment. It is only to a being in the intermediate stage, who can lay hold of some forms of energy while others elude his grasp, that energy appears to be passing inevitably from the available tothe dissipated state."
<span style="font-style:italic;">James Clerk Maxwell, "Diffusion", Encyclopaedia Brittanica, 1878</span>]]></content:encoded>
<wp:post_id>62</wp:post_id>
<wp:post_date>2007-02-23 17:08:21</wp:post_date>
<wp:post_date_gmt>2007-02-23 16:08:21</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>some-quotes-about-entropy</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
<wp:comment>
<wp:comment_id>67</wp:comment_id>
<wp:comment_author><![CDATA[nojhan]]></wp:comment_author>
<wp:comment_author_email>nojhan@gmail.com</wp:comment_author_email>
<wp:comment_author_url>http://nojhan.free.fr/metah</wp:comment_author_url>
<wp:comment_author_IP>193.49.172.146</wp:comment_author_IP>
<wp:comment_date>2007-02-26 10:44:09</wp:comment_date>
<wp:comment_date_gmt>2007-02-26 09:44:09</wp:comment_date_gmt>
<wp:comment_content><![CDATA[“Entropy is an anthropomorphic concept.”

I believe one can say exactly the same for the "emergence" concept. Entertaining :-)]]></wp:comment_content>
<wp:comment_approved>1</wp:comment_approved>
<wp:comment_type></wp:comment_type>
<wp:comment_parent>0</wp:comment_parent>
<wp:comment_user_id>0</wp:comment_user_id>
</wp:comment>
	</item>
<item>
<title>Ranking conferences</title>
<link>http://yamlb.wordpress.com/2007/03/09/ranking-conferences/</link>
<pubDate>Fri, 09 Mar 2007 14:47:24 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Work]]></category>

		<category domain="category" nicename="work"><![CDATA[Work]]></category>

		<category><![CDATA[research life]]></category>

		<category domain="category" nicename="research-life"><![CDATA[research life]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2007/03/09/ranking-conferences/</guid>
<description></description>
<content:encoded><![CDATA[In the machine learning/artificial intelligence field, conferences are as important as journals. Maybe even more important.

Having a good publication list is crutial for a researcher, for getting a PhD degree, finding a postdoc, a job, a grant, being promoted, becoming famous and ultimately getting married. (I'm not sure for the last one, does anybody have ever dated a grirl thanks to his publication history?).

Anyway, "recuiters" have to rank candidates. So they have to rank conferences. Such a conference ranking will always be fuzzy and controversial, but here are several temptatives. The first one, done by autralian university seems to me the most serious.  The "Top 6 list" are in the field of machine learning/Artifical intelligence/computer vision/pattern recognition depending on the ranking method.
<p align="center"><strong>Organisation:</strong></p>

<ol>
	<li>Link to ranking</li>
	<li>authors</li>
	<li>quotations from the website</li>
	<li>my comments</li>
	<li>Top6</li>
	<li>Criteria</li>
</ol>
<p align="center"><strong>Rankings:</strong></p>

<ol>
	<li><a href="http://www.infotech.monash.edu.au/research/internal-info/ranking-list/ict-conference-rankings-2.html">link</a></li>
	<li>Done by College of Engineering and Computer Science (CECS) at the ANU and National ICT Australia (NICTA)</li>
	<li>"NICTA and ANU are coordinating an Australia wide effort to develop a ranking for the ICT  conferences, typically attended by Australian academics in Computing and Electrical Engineering."
"<span style="font-weight:bold;">[...] the main mode of research dissemination is through conferences and not journal literature."</span> John W. Lloyd
"it may be valuable to have robust indicators from conference data for appointments and promotions."John W. Lloyd</li>
	<li>Seems more seriours
Different subfilds, including robotics</li>
	<li>Top 6 machine learning : AAAI AAMAS ACL AIED CADE COLT
Top 6 robotics : CDC ICARCV ICASSP IEEECDC FSR ICRA IEEEIV</li>
	<li>Criteria</li>
<ul>
	<li>reputation in its field</li>
	<li>importance of paper for the field</li>
	<li>researchers boast about getting accepted</li>
	<li>Low Acceptance rates</li>
	<li>leading program committee from top institutions</li>
</ul>
</ol>
...
<ol>
	<li><a href="http://www-static.cc.gatech.edu/~guofei/CS_ConfRank.htm">link</a></li>
	<li>done by a phD student Guofei Gu, Georgia Tech, USA</li>
	<li>"This is not an OFFICIAL, nor ACCURATE list. ONLY FOR REFERENCE."
<span style="font-weight:bold;">"As we all know, conference is more important than journal in computer science area."</span></li>
	<li>maybe outdated</li>
	<li>Top 6 machine learning : AAAI CVPR IJCAI ICCV ICML KDD</li>
	<li>Criteria :</li>
<ul>
	<li>Acceptance ratio</li>
	<li>Paper quality and impact</li>
	<li>Committee member quality</li>
	<li>Attendee/Paper number ratio</li>
	<li>Location</li>
	<li>History</li>
	<li>Industry connection</li>
</ul>
</ol>
...
<ol>
	<li><a href="http://www.cs.ualberta.ca/%7Ezaiane/htmldocs/ConfRanking.html">link</a></li>
	<li>Done by Osmar R. Zaïane, Ph.D., Associate Professor, univ of Alberta, Canada</li>
<li>"Top conferences [...] should be equivalent, if not superior in impact and prestige, to reputable journals."</li>
	<li>Several subfields</li>
	<li>Top 6 machine learning : IJCAI AAAI ICML UAI UM NIPS</li>
	<li>Criteria</li>
<ul>
	<li>general reputation</li>
	<li>citation of the papers published</li>
	<li>reputation of program committee</li>
	<li>reputation of the review process</li>
</ul>
</ol>
...
<ol>
	<li><a href="http://www.cs-conference-ranking.org/conferencerankings/topicsii.html">link</a></li>
	<li>Done by unknown</li>
	<li>research work, without any garanties
seems to rely on SVMs
proposes also a journal ranks</li>
	<li>Top 6 machine learning : AAAI NIPS IJCAI ICCV CVPR ICAA ICML</li>
	<li>Criteria</li>
<ul>
	<li>EIC (=Estimated Impact of Conference)<span style="font-weight:bold;"> not defined on the site.
</span></li>
</ul>
</ol>]]></content:encoded>
<wp:post_id>63</wp:post_id>
<wp:post_date>2007-03-09 15:47:24</wp:post_date>
<wp:post_date_gmt>2007-03-09 14:47:24</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>ranking-conferences</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>Link to real blog</title>
<link>http://yamlb.wordpress.com/2007/03/14/3/</link>
<pubDate>Wed, 14 Mar 2007 14:35:28 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Uncategorized]]></category>

		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>

<guid isPermaLink="false">http://yamlb.wordpress.com/2007/03/14/3/</guid>
<description></description>
<content:encoded><![CDATA[<em><strong>ALL THE POSTS OLDER THAN THIS ONE are copied from</strong></em>

<a href="http://emotion.inrialpes.fr/~dangauthier/blog" title="blog">http://emotion.inrialpes.fr/~dangauthier/blog</a>

Please go there.

This address ( http://yamlb.wordpress.com) will be used later (fall 2007)

Pierre - $latex P(\Pi | \Delta)$]]></content:encoded>
<wp:post_id>3</wp:post_id>
<wp:post_date>2007-03-14 14:35:28</wp:post_date>
<wp:post_date_gmt>2007-03-14 14:35:28</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>3</wp:post_name>
<wp:status>publish</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>post</wp:post_type>
<wp:post_password></wp:post_password>
	</item>
<item>
<title>wordpress2007-03-14xml</title>
<link>http://yamlb.wordpress.com/?attachment_id=4</link>
<pubDate>Wed, 14 Mar 2007 15:12:13 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Uncategorized]]></category>

		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>

<guid isPermaLink="false">http://yamlb.files.wordpress.com/2007/03/wordpress2007-03-14xml</guid>
<description></description>
<content:encoded><![CDATA[http://yamlb.files.wordpress.com/2007/03/wordpress2007-03-14xml]]></content:encoded>
<wp:post_id>4</wp:post_id>
<wp:post_date>2007-03-14 15:12:13</wp:post_date>
<wp:post_date_gmt>2007-03-14 15:12:13</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>wordpress2007-03-14xml</wp:post_name>
<wp:status>inherit</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>attachment</wp:post_type>
<wp:post_password></wp:post_password>
<wp:attachment_url>http://yamlb.wordpress.com/files/2007/03/wordpress2007-03-14xml</wp:attachment_url>
<wp:postmeta>
<wp:meta_key>_wp_attached_file</wp:meta_key>
<wp:meta_value>/home/wpcom/public_html/wp-content/blogs.dir/c88/874855/files/2007/03/wordpress2007-03-14xml</wp:meta_value>
</wp:postmeta>
	</item>
<item>
<title>wordpress2007-03-14xml</title>
<link>http://yamlb.wordpress.com/?attachment_id=5</link>
<pubDate>Wed, 14 Mar 2007 15:12:22 +0000</pubDate>
<dc:creator><![CDATA[Pierre]]></dc:creator>

		<category><![CDATA[Uncategorized]]></category>

		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>

<guid isPermaLink="false">http://yamlb.files.wordpress.com/2007/03/wordpress2007-03-14xml</guid>
<description></description>
<content:encoded><![CDATA[http://yamlb.files.wordpress.com/2007/03/wordpress2007-03-14xml]]></content:encoded>
<wp:post_id>5</wp:post_id>
<wp:post_date>2007-03-14 15:12:22</wp:post_date>
<wp:post_date_gmt>2007-03-14 15:12:22</wp:post_date_gmt>
<wp:comment_status>open</wp:comment_status>
<wp:ping_status>open</wp:ping_status>
<wp:post_name>wordpress2007-03-14xml-2</wp:post_name>
<wp:status>inherit</wp:status>
<wp:post_parent>0</wp:post_parent>
<wp:menu_order>0</wp:menu_order>
<wp:post_type>attachment</wp:post_type>
<wp:post_password></wp:post_password>
<wp:attachment_url>http://yamlb.wordpress.com/files/2007/03/wordpress2007-03-14xml</wp:attachment_url>
<wp:postmeta>
<wp:meta_key>_wp_attached_file</wp:meta_key>
<wp:meta_value>/home/wpcom/public_html/wp-content/blogs.dir/c88/874855/files/2007/03/wordpress2007-03-14xml</wp:meta_value>
</wp:postmeta>
	</item>
</channel>
</rss>
